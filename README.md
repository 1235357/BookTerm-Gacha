<h1><p align="center">ğŸ“š BookTerm Gacha</p></h1>
<p align="center"><strong>An LLM-Powered Agent for Automated Book Terminology Extraction</strong></p>
<p align="center"><em>Multi-Platform LLM Support with API Key Rotation - Extract Character & Location Names from Japanese Literatures</em></p>

<p align="center">
  <img src="https://img.shields.io/badge/Version-0.2.0-brightgreen.svg" alt="Version"/>
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python"/>
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License"/>
  <img src="https://img.shields.io/badge/LLM-Multi_Platform-orange.svg" alt="Multi Platform"/>
  <img src="https://img.shields.io/badge/NVIDIA-DeepSeek_V3.2-76B900.svg" alt="NVIDIA"/>
  <img src="https://img.shields.io/badge/ModelScope-é˜¿é‡Œäº‘ç™¾ç‚¼-blue.svg" alt="ModelScope"/>
  <img src="https://img.shields.io/badge/Zhipu-GLM_4.6v-red.svg" alt="Zhipu GLM"/>
  <img src="https://img.shields.io/badge/NER-BERT-purple.svg" alt="BERT NER"/>
  <img src="https://img.shields.io/badge/GPU-CUDA_Supported-76B900.svg" alt="CUDA"/>
</p>

---

## ğŸ“‹ Table of Contents

- [What is BookTerm Gacha?](#-what-is-bookterm-gacha)
- [Key Features](#-key-features)
- [Quick Start Guide](#-quick-start-guide)
- [Installation Methods](#-installation-methods)
  - [Method 1: Download Release (Recommended for Most Users)](#method-1-download-release-recommended-for-most-users)
  - [Method 2: Clone Repository (For Developers & GPU Users)](#method-2-clone-repository-for-developers--gpu-users)
- [Configuration Guide](#-configuration-guide)
- [How to Use](#-how-to-use)
- [Understanding the Output](#-understanding-the-output)
- [Troubleshooting](#-troubleshooting)
- [Technical Details](#-technical-details)
- [Release Notes](#-release-notes)
- [Acknowledgments](#-acknowledgments)
- [License](#-license)

---

## ğŸ¯ What is BookTerm Gacha?

**BookTerm Gacha** is an intelligent tool that automatically extracts character names, location names, and other important terminology from books and generates translation glossaries.

### The Problem It Solves

When translating Japanese Literatures to Chinese, translators face a common challenge:

> **How do you consistently translate character names like `ã‚¢ãƒªã‚¹`, `ãƒˆãƒªã‚·ãƒ¥ãƒ¼ãƒ©`, or `ãƒ†ã‚£ãƒŠãƒ¼ã‚·ãƒ£`?**

These katakana names need to be transliterated into Chinese, and keeping them consistent throughout a book (or book series) is tedious and error-prone.

### The Solution

BookTerm Gacha automates this process:

1. **Reads** your EPUB/TXT/MD book files
2. **Extracts** all character and location names using AI (BERT NER model)
3. **Analyzes** each name with context using LLM (Zhipu GLM)
4. **Generates** a terminology glossary ready for use with translation tools

### Who Is This For?

- ğŸ“– **Japanese Literatures Translators** - Create consistent terminology tables for your translation projects
- ğŸ® **Fan Translation Groups** - Standardize character names across team members
- ğŸ“š **Translation Tool Users** - Generate glossaries for [LinguaGacha](https://github.com/neavo/LinguaGacha), [GalTransl](https://github.com/xd2333/GalTransl), and similar tools
- ğŸ¤– **AI/LLM Enthusiasts** - Learn how to build practical LLM Agent applications

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ“– **Book-Focused** | Optimized specifically for EPUB, TXT, and MD book formats |
| ğŸ‡¯ğŸ‡µ **Japanese Optimized** | Fine-tuned for Japanese â†’ Chinese translation with zero kana residue |
| ï¿½ **Multi-Platform LLM** | Support for NVIDIA Build, ModelScope (é˜¿é‡Œäº‘ç™¾ç‚¼), Zhipu GLM, and more |
| ğŸ”„ **API Key Rotation** | Multiple API keys with automatic round-robin polling for massive throughput |
| ğŸš« **Smart Blacklist** | Automatic detection and blacklisting of banned/expired API keys |
| ğŸ†“ **Free Tier Options** | Zhipu GLM-4-Flash FREE, NVIDIA/ModelScope generous free quotas |
| ğŸš€ **GPU Acceleration** | Automatic CUDA detection for fast NER processing (optional) |
| ğŸ’¡ **Deep Thinking** | Support for reasoning models (DeepSeek R1, GLM with thinking) |
| ğŸ“Š **Rich Progress** | Beautiful progress bars showing exactly what's happening |
| âœ… **Quality Validation** | Automatic result checking for kana residue and issues |
| ğŸ“ **Multiple Formats** | Outputs in JSON dictionary, LinguaGacha, and GalTransl formats |

---

## ğŸš€ Quick Start Guide

**For users who just want to get started quickly:**

1. Download the latest release from [GitHub Releases](https://github.com/1235357/BookTermGacha/releases)
2. Extract the ZIP file to any folder
3. Choose your LLM platform and get API key(s):
   - **NVIDIA Build** (Recommended): [build.nvidia.com](https://build.nvidia.com/) - DeepSeek V3.2 å…è´¹é¢åº¦
   - **ModelScope é˜¿é‡Œäº‘ç™¾ç‚¼**: [modelscope.cn](https://www.modelscope.cn/) - å…è´¹é¢åº¦
   - **Zhipu AI æ™ºè°±**: [bigmodel.cn](https://bigmodel.cn/) - GLM-4-Flash å®Œå…¨å…è´¹
4. Edit `config.json` - add your API keys and select platform
5. Put your EPUB/TXT files in the `input/` folder
6. Run `app.exe`
7. Find your terminology glossary in the `output/` folder

**That's it!** For detailed instructions, continue reading below.

---

## ğŸ“¦ Installation Methods

There are **two ways** to use BookTerm Gacha. Choose the one that fits your needs:

| Method | Best For | GPU Support | Difficulty |
|--------|----------|-------------|------------|
| **[Method 1: Download Release](#method-1-download-release-recommended-for-most-users)** | Most users, quick setup | CPU only (bundled) | â­ Easy |
| **[Method 2: Clone Repository](#method-2-clone-repository-for-developers--gpu-users)** | Developers, GPU users | Full CUDA support | â­â­â­ Advanced |

---

### Method 1: Download Release (Recommended for Most Users)

This is the **easiest way** to get started. The release package includes everything you need.

#### Step 1: Download the Release

1. Go to [GitHub Releases](https://github.com/1235357/BookTermGacha/releases)
2. Download the latest `BookTermGacha-v0.1.0.zip` file
3. Extract the ZIP to any folder (e.g., `C:\BookTermGacha\` or `D:\Tools\BookTermGacha\`)

#### Step 2: Understand the Folder Structure

After extraction, you'll see:

```
BookTermGacha/
â”œâ”€â”€ app.exe                 # Main executable - double-click to run
â”œâ”€â”€ config.json             # Configuration file - YOU NEED TO EDIT THIS
â”œâ”€â”€ version.txt             # Version information
â”‚
â”œâ”€â”€ blacklist/              # Filter lists (pre-configured, don't modify)
â”‚   â”œâ”€â”€ jp_è¯­æ°”åŠ©è¯.json
â”‚   â”œâ”€â”€ jp_äººç§°ä»£è¯.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ prompt/                 # LLM prompts (pre-configured, don't modify)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ resource/               # Required resources
â”‚   â”œâ”€â”€ kg_ner_bf16/        # BERT NER model (DO NOT DELETE)
â”‚   â””â”€â”€ llm_config/         # LLM configuration presets
â”‚
â”œâ”€â”€ input/                  # PUT YOUR BOOKS HERE
â”‚   â””â”€â”€ (empty - add your EPUB/TXT files)
â”‚
â”œâ”€â”€ output/                 # RESULTS APPEAR HERE
â”‚   â””â”€â”€ (empty - generated files will be here)
â”‚
â””â”€â”€ log/                    # Log files for troubleshooting
    â””â”€â”€ (auto-generated)
```

#### Step 3: Get Your API Keys

BookTerm Gacha æ”¯æŒå¤šä¸ª LLM å¹³å°ï¼Œå¯ä»¥é…ç½®å¤šä¸ª API Key è¿›è¡Œè½®è¯¢ä»¥è·å¾—æ›´é«˜ååé‡ï¼š

**ğŸ† æ¨èå¹³å°å¯¹æ¯”:**

| å¹³å° | æ¨¡å‹ | å…è´¹é¢åº¦ | é€Ÿåº¦ | è´¨é‡ | æ¨èåº¦ |
|------|------|----------|------|------|--------|
| **NVIDIA Build** | DeepSeek V3.2 | 1000 æ¬¡/å¤© | â­â­â­â­â­ | â­â­â­â­â­ | ğŸ¥‡ é¦–é€‰ |
| **ModelScope é˜¿é‡Œäº‘ç™¾ç‚¼** | DeepSeek V3.2/R1 | å……è¶³ | â­â­â­â­ | â­â­â­â­â­ | ğŸ¥ˆ å¤‡é€‰ |
| **Zhipu AI æ™ºè°±** | GLM-4.6v-Flash | æ— é™åˆ¶ | â­â­â­ | â­â­â­â­ | ğŸ¥‰ å…è´¹é¦–é€‰ |

**è·å– API Key:**

**NVIDIA Build (æ¨è):**
1. è®¿é—® [https://build.nvidia.com/](https://build.nvidia.com/)
2. æ³¨å†Œ/ç™»å½• NVIDIA è´¦å·
3. æœç´¢ "DeepSeek V3.2" æ¨¡å‹
4. ç‚¹å‡» "Get API Key" è·å–å¯†é’¥
5. å¯æ³¨å†Œå¤šä¸ªè´¦å·è·å–å¤šä¸ª Key ç”¨äºè½®è¯¢

**ModelScope é˜¿é‡Œäº‘ç™¾ç‚¼:**
1. è®¿é—® [https://www.modelscope.cn/](https://www.modelscope.cn/)
2. ä½¿ç”¨æ”¯ä»˜å®/é˜¿é‡Œäº‘è´¦å·ç™»å½•
3. è¿›å…¥æ¨¡å‹æ¨ç†é¡µé¢
4. è·å– API Key (æ ¼å¼: `ms-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`)

**Zhipu AI æ™ºè°± (å®Œå…¨å…è´¹):**
1. è®¿é—® [https://bigmodel.cn/](https://bigmodel.cn/)
2. æ³¨å†Œè´¦å· (éœ€æ‰‹æœºéªŒè¯)
3. è¿›å…¥æ§åˆ¶å°åˆ›å»º API Key

> ğŸ’¡ **æ€§èƒ½æç¤º**: ä½¿ç”¨å¤šä¸ª API Key è½®è¯¢å¯ä»¥å¤§å¹…æå‡å¤„ç†é€Ÿåº¦ï¼å»ºè®®æ¯ä¸ªå¹³å°å‡†å¤‡ 3-5 ä¸ª Keyã€‚

#### Step 4: Configure Your API Keys

1. Open `config.json` with any text editor (Notepad, VS Code, etc.)
2. The new multi-platform configuration format:

```json
{
    "activate_platform": 1,
    "platforms": [
        {
            "id": 0,
            "name": "æ™ºè°±GLM-4.6v-flash(å…è´¹)",
            "api_url": "https://open.bigmodel.cn/api/paas/v4",
            "api_key": ["your-zhipu-api-key"],
            "model": "glm-4.6v-flash",
            "thinking": true,
            "description": "æ™ºè°±å…è´¹æ¨¡å‹ï¼Œæ”¯æŒæ·±åº¦æ€è€ƒ"
        },
        {
            "id": 1,
            "name": "NVIDIA-DeepSeek-V3.2",
            "api_url": "https://integrate.api.nvidia.com/v1",
            "api_key": [
                "nvapi-key1",
                "nvapi-key2",
                "nvapi-key3"
            ],
            "model": "deepseek-ai/deepseek-v3.2",
            "thinking": true,
            "description": "NVIDIA Build DeepSeek V3.2ï¼Œå¤šKeyè½®è¯¢"
        },
        {
            "id": 2,
            "name": "é­”å¡”-DeepSeek-V3.2",
            "api_url": "https://api-inference.modelscope.cn/v1/",
            "api_key": [
                "ms-key1",
                "ms-key2"
            ],
            "model": "deepseek-ai/DeepSeek-V3.2",
            "thinking": true,
            "description": "é˜¿é‡Œäº‘ç™¾ç‚¼ ModelScope"
        }
    ]
}
```

3. **è®¾ç½® `activate_platform`** ä¸ºä½ æƒ³ä½¿ç”¨çš„å¹³å° ID:
   - `0` = æ™ºè°± GLM (å…è´¹)
   - `1` = NVIDIA DeepSeek V3.2 (æ¨è)
   - `2` = ModelScope DeepSeek V3.2
   - `3` = ModelScope DeepSeek R1

4. **æ·»åŠ ä½ çš„ API Keys** åˆ°å¯¹åº”å¹³å°çš„ `api_key` æ•°ç»„ä¸­

5. **Save** the file

#### Step 5: Add Your Books

1. Copy your Japanese book files into the `input/` folder
2. Supported formats:
   - `.epub` - E-book format (recommended)
   - `.txt` - Plain text (must be UTF-8 encoded)
   - `.md` - Markdown files

#### Step 6: Run the Program

1. **Double-click** `app.exe` to start
2. A console window will open showing progress
3. Wait for processing to complete (time depends on book size)
4. Check the `output/` folder for your results

#### What If It Doesn't Work?

If you encounter errors:
- See the [Troubleshooting](#-troubleshooting) section
- Check the `log/` folder for detailed error messages
- If GPU-related issues occur, consider [Method 2](#method-2-clone-repository-for-developers--gpu-users)

---

### Method 2: Clone Repository (For Developers & GPU Users)

Remember to clone https://huggingface.co/neavo/keyword_gacha_multilingual_ner to "\BookTerm Gacha\resource\kg_ner_bf16" (Since â€œmodel.safetensorsâ€ is too large for GitHub)

Choose this method if:
- âœ… You have an NVIDIA GPU and want **faster processing** (3-10x speedup)
- âœ… The release version **doesn't recognize your GPU**
- âœ… You want to **modify the code** or contribute to development
- âœ… You want to use a **different Python version** or environment
- âœ… You're experiencing **compatibility issues** with the release version

#### Prerequisites

Before starting, make sure you have:

| Requirement | How to Check | How to Install |
|-------------|--------------|----------------|
| **Python 3.10+** | `python --version` | [python.org](https://www.python.org/downloads/) |
| **Git** (optional) | `git --version` | [git-scm.com](https://git-scm.com/) |
| **NVIDIA GPU** (optional) | `nvidia-smi` | Driver from [nvidia.com](https://www.nvidia.com/drivers/) |
| **CUDA Toolkit** (for GPU) | `nvcc --version` | [CUDA Downloads](https://developer.nvidia.com/cuda-downloads) |

#### Step 1: Clone or Download the Repository

**Option A: Using Git (Recommended)**
```bash
# Open Command Prompt or PowerShell
git clone https://github.com/1235357/BookTermGacha.git
cd BookTermGacha
```

**Option B: Download ZIP**
1. Go to the repository page
2. Click "Code" â†’ "Download ZIP"
3. Extract to your preferred location
4. Open Command Prompt/PowerShell and navigate to the folder:
```bash
cd C:\path\to\BookTermGacha
```

#### Step 2: Create a Virtual Environment (Highly Recommended)

A virtual environment keeps this project's dependencies separate from other Python projects.

**Windows (Command Prompt):**
```bash
# Create virtual environment
python -m venv venv

# Activate it
venv\Scripts\activate

# You should see (venv) at the start of your command line
```

**Windows (PowerShell):**
```powershell
# Create virtual environment
python -m venv venv

# Activate it (you may need to allow script execution first)
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
.\venv\Scripts\Activate.ps1
```

**Linux/Mac:**
```bash
# Create virtual environment
python3 -m venv venv

# Activate it
source venv/bin/activate
```

> âš ï¸ **Important**: Always activate the virtual environment before running commands!

#### Step 3: Install PyTorch (CRITICAL for GPU Users)

This is the **most important step** for GPU acceleration. You must install PyTorch with the correct CUDA version **BEFORE** installing other dependencies.

**First, check your CUDA version:**
```bash
nvidia-smi
```

Look for "CUDA Version" in the output (e.g., "CUDA Version: 12.4").

**Then install PyTorch with matching CUDA:**

| Your CUDA Version | Installation Command |
|-------------------|---------------------|
| **CUDA 12.6** (RTX 40 series) | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126` |
| **CUDA 12.4** | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124` |
| **CUDA 12.1** | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121` |
| **CUDA 11.8** (GTX 10/16/20 series) | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118` |
| **No GPU / CPU Only** | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu` |

**Example for CUDA 12.4:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

#### Step 4: Install Other Dependencies

```bash
pip install -r requirements.txt
```

This will install all required packages:
- `transformers` - For BERT NER model
- `openai` - For LLM API calls
- `ebooklib` - For EPUB reading
- `rich` - For beautiful console output
- `pykakasi` - For Japanese text processing
- And more...

#### Step 5: Verify Your Installation

Run these commands to make sure everything is working:

```bash
# Check Python version
python --version

# Check if PyTorch sees your GPU
python -c "import torch; print(f'PyTorch Version: {torch.__version__}')"
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\"}')"

# Check transformers
python -c "from transformers import AutoModel; print('Transformers: OK')"

# Check other dependencies
python -c "import openai, ebooklib, rich; print('All dependencies: OK')"
```

**Expected output (with GPU):**
```
PyTorch Version: 2.5.1+cu124
CUDA Available: True
GPU Device: NVIDIA GeForce RTX 4090
Transformers: OK
All dependencies: OK
```

**Expected output (CPU only):**
```
PyTorch Version: 2.5.1+cpu
CUDA Available: False
GPU Device: CPU only
Transformers: OK
All dependencies: OK
```

#### Step 6: Configure Your API Key

Same as Method 1 - edit `config.json` with your Zhipu AI API key.

#### Step 7: Run the Program

```bash
# Make sure virtual environment is activated
# (venv) should appear in your prompt

python app.py
```

#### Common Issues and Solutions

| Issue | Solution |
|-------|----------|
| `CUDA Available: False` even with GPU | Reinstall PyTorch with correct CUDA version |
| `ModuleNotFoundError` | Make sure venv is activated, run `pip install -r requirements.txt` |
| Permission errors on Windows | Run Command Prompt as Administrator |
| Python not found | Add Python to PATH or use full path |

---

## âš™ï¸ Configuration Guide

The `config.json` file controls all settings. Here's a detailed explanation:

### Platform Configuration (æ ¸å¿ƒé…ç½®)

```json
{
    "activate_platform": 1,
    "platforms": [...]
}
```

| Setting | Description | Values |
|---------|-------------|--------|
| `activate_platform` | å½“å‰æ¿€æ´»çš„å¹³å° ID | `0`, `1`, `2`, `3` |
| `platforms` | å¹³å°é…ç½®æ•°ç»„ | è§ä¸‹æ–¹è¯¦ç»†è¯´æ˜ |

### Platform Object Structure (å¹³å°é…ç½®ç»“æ„)

```json
{
    "id": 1,
    "name": "NVIDIA-DeepSeek-V3.2",
    "api_url": "https://integrate.api.nvidia.com/v1",
    "api_key": ["key1", "key2", "key3"],
    "model": "deepseek-ai/deepseek-v3.2",
    "thinking": true,
    "top_p": 0.95,
    "temperature": 0.95,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0,
    "description": "æè¿°æ–‡å­—"
}
```

| Field | Description | Example |
|-------|-------------|---------|
| `id` | å¹³å°å”¯ä¸€æ ‡è¯† | `0`, `1`, `2`, `3` |
| `name` | å¹³å°æ˜¾ç¤ºåç§° | `"NVIDIA-DeepSeek-V3.2"` |
| `api_url` | API ç«¯ç‚¹ URL | `"https://integrate.api.nvidia.com/v1"` |
| `api_key` | API Key æ•°ç»„ (æ”¯æŒå¤š Key è½®è¯¢) | `["key1", "key2"]` |
| `model` | æ¨¡å‹åç§° | `"deepseek-ai/deepseek-v3.2"` |
| `thinking` | æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ | `true` / `false` |

### Pre-configured Platforms (é¢„é…ç½®å¹³å°)

| ID | Platform | API URL | Model | Free Tier |
|----|----------|---------|-------|-----------|
| `0` | **æ™ºè°± GLM** | `https://open.bigmodel.cn/api/paas/v4` | `glm-4.6v-flash` | âœ… å®Œå…¨å…è´¹ |
| `1` | **NVIDIA Build** | `https://integrate.api.nvidia.com/v1` | `deepseek-ai/deepseek-v3.2` | âœ… 1000æ¬¡/å¤© |
| `2` | **ModelScope V3.2** | `https://api-inference.modelscope.cn/v1/` | `deepseek-ai/DeepSeek-V3.2` | âœ… å……è¶³é¢åº¦ |
| `3` | **ModelScope R1** | `https://api-inference.modelscope.cn/v1/` | `deepseek-ai/DeepSeek-R1-0528` | âœ… å……è¶³é¢åº¦ |

### Multi-API Key Rotation (å¤š Key è½®è¯¢)

```json
"api_key": [
    "nvapi-key1-xxxxx",
    "nvapi-key2-xxxxx",
    "nvapi-key3-xxxxx",
    "nvapi-key4-xxxxx",
    "nvapi-key5-xxxxx"
]
```

**ç‰¹æ€§ï¼š**
- ğŸ”„ **è‡ªåŠ¨è½®è¯¢**: è¯·æ±‚è‡ªåŠ¨åˆ†é…åˆ°ä¸åŒçš„ API Key
- ğŸš« **æ™ºèƒ½é»‘åå•**: è¢«å°ç¦çš„ Key è‡ªåŠ¨åŠ å…¥é»‘åå•ï¼Œä¸å½±å“å…¶ä»– Key
- âš¡ **å¹¶å‘æå‡**: 5 ä¸ª Key = 5 å€ååé‡
- ğŸ“Š **çŠ¶æ€æ˜¾ç¤º**: å¯åŠ¨æ—¶æ˜¾ç¤ºå¯ç”¨ Key æ•°é‡

### Optional Settings (å¯é€‰é…ç½®)

```json
{
    "count_threshold": [2, "å‡ºç°æ¬¡æ•°é˜ˆå€¼"],
    "score_threshold": [0.60, "NER ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0)"],
    "max_context_samples": [5, "ä¸Šä¸‹æ–‡é‡‡æ ·æ®µè½æ•°"],
    "tokens_per_sample": [512, "æ¯æ®µæœ€å¤§ token æ•°"],
    "ner_target_types": [["PER", "LOC"], "æå–çš„å®ä½“ç±»å‹"],
    "request_timeout": [1800, "API è¶…æ—¶æ—¶é—´(ç§’)"],
    "stream_first_chunk_timeout_seconds": [600, "é¦–åŒ…ç­‰å¾…è¶…æ—¶(ç§’)ï¼šä»â€œå‘â€åˆ°â€œæ€/æ”¶â€"],
    "stream_stall_timeout_seconds": [120, "æµå¼å¡ä½è¶…æ—¶(ç§’)ï¼šå·²æœ‰ chunk ä½†é•¿æ—¶é—´æ— æ–°æ•°æ®"],
    "stream_retry_attempts": [3, "æµå¼é‡è¯•æ¬¡æ•°(åŒ…å«é¦–æ¬¡å°è¯•)"],
    "stream_retry_backoff_seconds": [2, "æµå¼é‡è¯•é€€é¿åŸºå‡†ç§’æ•°(çº¿æ€§é€€é¿)"],
    "llamacpp_auto_detect_enable": [true, "æ˜¯å¦è‡ªåŠ¨æ£€æµ‹ llama.cpp(/slots) å¹¶è‡ªåŠ¨è®¾ç½®é¢‘ç‡é˜ˆå€¼"],
    "request_frequency_auto_downgrade_enable": [false, "æ˜¯å¦å¯ç”¨é«˜é¢‘è¯·æ±‚è‡ªåŠ¨é™çº§(é¿å…429)"],
    "request_frequency_auto_downgrade_threshold": [20, "è§¦å‘è‡ªåŠ¨é™çº§çš„é¢‘ç‡é˜ˆå€¼"],
    "request_frequency_auto_downgrade_to": [10, "è‡ªåŠ¨é™çº§åçš„é¢‘ç‡é˜ˆå€¼"],
    "request_frequency_threshold": [10, "æ¯ç§’æœ€å¤§è¯·æ±‚æ•°"],
    "max_concurrent_requests": [90, "æœ€å¤§å¹¶å‘è¯·æ±‚æ•°"],
    "traditional_chinese_enable": [false, "ç¹ä½“ä¸­æ–‡è¾“å‡º"]
}
```

| Setting | Description | Default | æ¨èå€¼ |
|---------|-------------|---------|--------|
| `count_threshold` | è¯è¯­æœ€å°‘å‡ºç°æ¬¡æ•° | `2` | ä¿æŒé»˜è®¤ |
| `score_threshold` | NER ç½®ä¿¡åº¦é˜ˆå€¼ | `0.60` | `0.50-0.70` |
| `stream_first_chunk_timeout_seconds` | é¦–åŒ…ç­‰å¾…è¶…æ—¶ | `600` | æœåŠ¡æ³¢åŠ¨å¤§å¯è°ƒå¤§ |
| `stream_stall_timeout_seconds` | æµå¼å¡ä½è¶…æ—¶ | `120` | 60-180 |
| `request_frequency_threshold` | æ¯ç§’è¯·æ±‚æ•°ä¸Šé™ | `10` | å¤š Key æ—¶è®¾ä¸º `5-10` |
| `max_concurrent_requests` | æœ€å¤§å¹¶å‘æ•° | `90` | å¤š Key æ—¶å¯å¢åŠ  |
| `traditional_chinese_enable` | ç¹ä½“ä¸­æ–‡ | `false` | å°æ¹¾/é¦™æ¸¯ç”¨æˆ·è®¾ä¸º `true` |

### Adding Custom LLM Providers (æ·»åŠ è‡ªå®šä¹‰å¹³å°)

å¯ä»¥åœ¨ `platforms` æ•°ç»„ä¸­æ·»åŠ ä»»ä½• OpenAI å…¼å®¹çš„ APIï¼š

```json
{
    "id": 4,
    "name": "My-Custom-Provider",
    "api_url": "https://api.example.com/v1",
    "api_key": ["your-api-key"],
    "model": "model-name",
    "thinking": false,
    "top_p": 0.95,
    "temperature": 0.7,
    "description": "è‡ªå®šä¹‰å¹³å°"
}
```

**æ”¯æŒçš„å¹³å°ç±»å‹ï¼š**

| Provider | Base URL | Model | Notes |
|----------|----------|-------|-------|
| **NVIDIA Build** | `https://integrate.api.nvidia.com/v1` | `deepseek-ai/deepseek-v3.2` | ğŸ¥‡ æ¨èï¼Œæ”¯æŒæ€è€ƒæ¨¡å¼ |
| **ModelScope** | `https://api-inference.modelscope.cn/v1/` | `deepseek-ai/DeepSeek-V3.2` | ğŸ¥ˆ é˜¿é‡Œäº‘ç™¾ç‚¼ |
| **Zhipu AI** | `https://open.bigmodel.cn/api/paas/v4` | `glm-4.6v-flash` | ğŸ†“ å®Œå…¨å…è´¹ |
| **DeepSeek** | `https://api.deepseek.com/v1` | `deepseek-chat` | å®˜æ–¹ API |
| **OpenAI** | `https://api.openai.com/v1` | `gpt-4o-mini` | é«˜è´¨é‡ï¼Œé«˜æˆæœ¬ |
| **Local LLM** | `http://localhost:11434/v1` | varies | Ollama ç­‰æœ¬åœ°æ¨¡å‹ |

---

## ğŸ“– How to Use

### Step-by-Step Workflow

#### 1. Prepare Your Input Files

- Place your Japanese book files in the `input/` folder
- Supported formats: `.epub`, `.txt`, `.md`
- Multiple files can be processed in one run
- File names can be in any language (Japanese, English, Chinese, etc.)

**Example:**
```
input/
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1.epub
â”œâ”€â”€ Unnamed Memory.epub
â”œâ”€â”€ my_novel.txt
â””â”€â”€ another_book.md
```

#### 2. Run the Program

**Release version:** Double-click `app.exe`

**Development version:** 
```bash
python app.py
```

#### 3. Monitor Progress

The program shows real-time progress:

```
â•­â”€ BookTerm Gacha v0.1.0 â”€â•®
â”‚ Processing: è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1.epub
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[1/4] Loading book...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% Reading EPUB

[2/4] NER Entity Extraction (BERT)...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% Found 127 entities

[3/4] Context Translation (LLM)...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  42% Processing entity 54/127

[4/4] Semantic Analysis (LLM)...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% Analysis complete

âœ“ Processing complete! Check output/ folder.
```

#### 4. Collect Your Results

After processing, check the `output/` folder:

```
output/
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_è§’è‰²_è¯å…¸.json      # Character dictionary
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_è§’è‰²_æœ¯è¯­è¡¨.json    # LinguaGacha glossary
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_è§’è‰²_galtransl.txt  # GalTransl format
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_è§’è‰²_æ—¥å¿—.txt       # Detailed log
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_åœ°ç‚¹_è¯å…¸.json      # Location dictionary
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_åœ°ç‚¹_æœ¯è¯­è¡¨.json    # Location glossary
â””â”€â”€ çµæœæ¤œæŸ»_æŠ¥å‘Š.json                              # Quality report
```

---

## ğŸ“Š Understanding the Output

### Dictionary Format (`_è¯å…¸.json`)

The main output - a list of terms with translations:

```json
[
    {
        "src": "ãƒªãƒ ãƒ«",
        "dst": "åˆ©å§†é²",
        "info": "ä¸»è§’ï¼Œè½¬ç”Ÿæˆå²è±å§†çš„æ—¥æœ¬äººï¼Œåæˆä¸ºé­”ç‹ã€‚"
    },
    {
        "src": "ã‚·ã‚º",
        "dst": "é™",
        "info": "å¥³æ€§å†’é™©è€…ï¼Œè¢«å¬å”¤åˆ°å¼‚ä¸–ç•Œçš„æ—¥æœ¬äººã€‚"
    }
]
```

| Field | Description |
|-------|-------------|
| `src` | Original Japanese name (source) |
| `dst` | Chinese translation (destination) |
| `info` | Character description/summary |

### LinguaGacha Format (`_æœ¯è¯­è¡¨.json`)

Ready to import into [LinguaGacha](https://github.com/neavo/LinguaGacha):

```json
[
    {
        "src": "ãƒªãƒ ãƒ«",
        "dst": "åˆ©å§†é²",
        "info": "è§’è‰² - ç”· - ä¸»è§’ï¼Œè½¬ç”Ÿæˆå²è±å§†çš„æ—¥æœ¬äºº..."
    }
]
```

### GalTransl Format (`_galtransl.txt`)

For use with [GalTransl](https://github.com/xd2333/GalTransl):

```
ãƒªãƒ ãƒ« | åˆ©å§†é²
ã‚·ã‚º | é™
ãƒ´ã‚§ãƒ«ãƒ‰ãƒ© | ç»´é²å¤šæ‹‰
```

### Quality Report (`çµæœæ¤œæŸ»_æŠ¥å‘Š.json`)

Automatically checks for issues:

```json
{
    "å‡åæ®‹ç•™": ["ã‚¨ãƒ«ãƒ•ã®é‡Œ â†’ ç²¾çµã®é‡Œ"],
    "æœªç¿»è¯‘æ¡ç›®": ["ã‚¢ãƒ«ãƒ“ã‚¹"],
    "ç›¸ä¼¼åº¦é—®é¢˜": []
}
```

---

## ğŸ”§ Troubleshooting

### Common Problems and Solutions

#### "API key invalid" Error

**Problem:** The program says your API key is invalid.

**Solutions:**
1. Double-check your API key in `config.json`
2. Make sure there are no extra spaces
3. Verify the key is active on [bigmodel.cn](https://bigmodel.cn/)
4. Check if you've exceeded the free tier limits

#### GPU Not Detected (Release Version)

**Problem:** The release version runs on CPU even though you have an NVIDIA GPU.

**Why:** The release is bundled with CPU-only PyTorch for maximum compatibility.

**Solution:** Use [Method 2](#method-2-clone-repository-for-developers--gpu-users) to install with proper CUDA support.

#### "CUDA out of memory" Error

**Problem:** GPU runs out of memory during NER processing.

**Solutions:**
1. Close other GPU-intensive applications
2. Process smaller files or split large books
3. The program will automatically fall back to CPU if needed

#### "Module not found" Error

**Problem:** Python can't find required packages.

**Solutions:**
```bash
# Make sure venv is activated
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Reinstall dependencies
pip install -r requirements.txt
```

#### Slow Processing

**Tips to speed up:**
1. Use GPU (Method 2 installation)
2. Reduce `max_context_samples` in config
3. Increase `count_threshold` to process fewer entities
4. Process books one at a time

#### Japanese Text Displays as Garbled Characters

**Problem:** Output shows `???` or garbled text.

**Solutions:**
1. Make sure your terminal supports UTF-8
2. For Windows: Run `chcp 65001` before starting
3. Open output files with UTF-8 encoding (use VS Code or Notepad++)

---

## ğŸ”¬ Technical Details

### How It Works

BookTerm Gacha uses a **4-stage pipeline**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BookTerm Gacha Pipeline                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Stage 1: NER Extraction (BERT)                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  â€¢ Reads book text from EPUB/TXT/MD                         â”‚
â”‚  â€¢ BERT model identifies named entities                     â”‚
â”‚  â€¢ Filters: Only keeps names WITH kana characters           â”‚
â”‚  â€¢ Output: List of potential character/location names       â”‚
â”‚                                                             â”‚
â”‚                         â†“                                   â”‚
â”‚                                                             â”‚
â”‚  Stage 2: Context Sampling & Translation (LLM)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚  â€¢ For each entity, samples N context paragraphs            â”‚
â”‚  â€¢ LLM translates sampled context for better understanding  â”‚
â”‚  â€¢ Line count mismatch is tolerated (quality > alignment)   â”‚
â”‚                                                             â”‚
â”‚                         â†“                                   â”‚
â”‚                                                             â”‚
â”‚  Stage 3: LLM Analysis & Term Generation                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  â€¢ Sends entity + original context (+ translated context)   â”‚
â”‚  â€¢ LLM returns: translation, gender, category, summary      â”‚
â”‚  â€¢ Validation: Checks for kana residue, degradation         â”‚
â”‚  â€¢ Rolling retry: failed items are re-queued immediately    â”‚
â”‚                                                             â”‚
â”‚                         â†“                                   â”‚
â”‚                                                             â”‚
â”‚  Stage 4: Output Generation                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  â€¢ ResultChecker validates all entries                      â”‚
â”‚  â€¢ Generates multiple output formats                        â”‚
â”‚  â€¢ Creates quality report                                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why BERT + LLM?

| Component | Task | Why This Choice |
|-----------|------|-----------------|
| **BERT NER** | Find names in text | Fast, accurate, runs locally |
| **Zhipu GLM** | Translate & analyze | Better context understanding, creative translation |

Using both gives the **best of both worlds** - fast extraction with intelligent analysis.

### Smart Kana Handling

The system handles Japanese kana intelligently:

- **Strict Mode**: Flags any kana remaining in translations
- **Tolerance**: Allows certain kana (ã£, ãƒ¼, ãƒ¶) that appear in place names
- **Fallback**: After 5 failed retries, force-transliterates using romaji â†’ Chinese mapping

---

## ğŸ“‹ Release Notes

### Version 0.2.0 - Multi-Platform & API Key Rotation

**Release Date:** January 2026

**ğŸ‰ Major Version Upgrade!**

This release brings massive performance improvements with multi-platform LLM support and API key rotation.

#### âœ¨ New Features

- **ğŸŒ Multi-Platform Support**: NVIDIA Build, ModelScope (é˜¿é‡Œäº‘ç™¾ç‚¼), Zhipu GLM å…¨é¢æ”¯æŒ
- **ğŸ”„ API Key Rotation**: å¤š API Key è‡ªåŠ¨è½®è¯¢ï¼Œå¤§å¹…æå‡å¹¶å‘ååé‡
- **ğŸš« Smart Blacklist**: è‡ªåŠ¨æ£€æµ‹è¢«å°ç¦çš„ Key å¹¶åŠ å…¥é»‘åå•
- **ğŸ’¡ Deep Thinking Mode**: æ”¯æŒ DeepSeek V3.2/R1 çš„æ¨ç†æ¨¡å¼
- **âš¡ NVIDIA DeepSeek**: é¦–é€‰æ¨èå¹³å°ï¼Œé€Ÿåº¦ä¸è´¨é‡å…¼å…·
- **ğŸ“Š Enhanced Status**: å¯åŠ¨æ—¶æ˜¾ç¤ºå¹³å°ä¿¡æ¯å’Œå¯ç”¨ Key æ•°é‡

#### ğŸ”§ Technical Improvements

- å…¨æ–°çš„å¤šå¹³å°é…ç½®æ ¼å¼ (`platforms` æ•°ç»„)
- æ™ºèƒ½å¹³å°æ£€æµ‹ (NVIDIA/ModelScope/Zhipu è‡ªåŠ¨è¯†åˆ«)
- æµå¼å“åº”ä¼˜åŒ–ï¼Œæ”¯æŒ `reasoning_content` æå–
- API Key é»‘åå•æœºåˆ¶ (è‡ªåŠ¨å¤„ç† 403 é”™è¯¯)
- å¹¶å‘æ§åˆ¶ä¼˜åŒ– (`max_concurrent_requests` é…ç½®)

#### ğŸ“¦ Pre-configured Platforms

| Platform | Model | Features |
|----------|-------|----------|
| **æ™ºè°± GLM-4.6v-flash** | å…è´¹æ— é™åˆ¶ | æ·±åº¦æ€è€ƒ |
| **NVIDIA DeepSeek V3.2** | 5 Key è½®è¯¢ | é«˜é€Ÿæ¨ç† |
| **ModelScope DeepSeek V3.2** | 5 Key è½®è¯¢ | é˜¿é‡Œäº‘ç™¾ç‚¼ |
| **ModelScope DeepSeek R1** | æ¨ç†æ¨¡å‹ | æ·±åº¦æ€è€ƒ |

#### âš ï¸ Breaking Changes

- `config.json` æ ¼å¼å·²æ›´æ–°ä¸ºå¤šå¹³å°æ ¼å¼
- æ—§é…ç½®éœ€è¦è¿ç§»åˆ°æ–°çš„ `platforms` æ•°ç»„æ ¼å¼
- æ–°å¢ `activate_platform` å­—æ®µæŒ‡å®šæ´»åŠ¨å¹³å°

---

### Version 0.1.0-Zhipu_GLM-Optimize

**Release Date:** January 2026

**ğŸ‰ First Major Release!**

This is the first stable release of BookTerm Gacha, specifically optimized for the Zhipu GLM API.

#### âœ¨ New Features

- **Zhipu GLM Optimization**: Fine-tuned prompts and settings for best results with GLM-4-Flash
- **Complete Workflow**: Full pipeline from book input to glossary output
- **Multiple Output Formats**: JSON dictionary, LinguaGacha glossary, GalTransl format
- **Rich Progress Display**: Beautiful console output with progress bars
- **Quality Validation**: Automatic checking for kana residue and translation issues
- **Smart Retry Logic**: Intelligent retry with forced transliteration fallback

#### ğŸ”§ Technical Improvements

- Optimized NER filtering (only processes kana-containing entities)
- Place name particle handling (ãƒ¶, ã®, etc.)
- Context-aware kana detection with tolerance rules
- Comprehensive error handling and logging

#### ğŸ“¦ What's Included

- Pre-configured for Zhipu GLM API (free tier available)
- BERT NER model for Japanese entity extraction
- Blacklist filters for common words (particles, pronouns, etc.)
- LLM prompts optimized for terminology extraction

#### âš ï¸ Known Limitations

- Release version uses CPU-only PyTorch (use dev setup for GPU)
- Optimized for Japanese â†’ Chinese (other languages may work but untested)
- Large books (500k+ characters) may take 30+ minutes

#### ğŸ™ Based On

- [KeywordGacha v0.13.1](https://github.com/neavo/KeywordGacha) - Core workflow and NER model
- [LinguaGacha](https://github.com/neavo/LinguaGacha) - Validation patterns and text utilities

---

## ğŸ™ Acknowledgments

This project wouldn't be possible without:

- **[KeywordGacha](https://github.com/neavo/KeywordGacha)** by neavo - The original project that inspired this fork
- **[LinguaGacha](https://github.com/neavo/LinguaGacha)** by neavo - Design patterns and validation logic
- **[NVIDIA Build](https://build.nvidia.com/)** - High-performance DeepSeek API
- **[ModelScope é˜¿é‡Œäº‘ç™¾ç‚¼](https://www.modelscope.cn/)** - Generous free LLM API
- **[Zhipu AI / BigModel](https://bigmodel.cn/)** - Free LLM API that makes this accessible to everyone
- **[Hugging Face Transformers](https://huggingface.co/)** - The BERT NER pipeline
- **The Japanese Literatures Translation Community** - For feedback and testing

---

## ğŸ“„ License

This project is released under the **MIT License**.

You are free to:
- âœ… Use commercially
- âœ… Modify
- âœ… Distribute
- âœ… Use privately

With the condition that you include the original license and copyright notice.

**If you use this tool in your translation work, please credit appropriately.**

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. **Report Bugs**: Open an issue describing the problem
2. **Suggest Features**: Share your ideas for improvements
3. **Submit Code**: Fork, make changes, submit a pull request
4. **Improve Docs**: Help make this README even better
5. **Share**: Tell others about this tool!

### Development Setup

```bash
# Clone the repo
git clone https://github.com/1235357/BookTermGacha.git
cd BookTermGacha

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows

# Install with GPU support (see Method 2)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt

# Run tests
python -m pytest -v

# Make your changes and submit a PR!
```

---

## â“ FAQ

**Q: å“ªä¸ª LLM å¹³å°æœ€æ¨èï¼Ÿ**
A: NVIDIA Build çš„ DeepSeek V3.2 æ˜¯é¦–é€‰ï¼Œé€Ÿåº¦å¿«è´¨é‡é«˜ã€‚ModelScopeï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼‰æ˜¯å¤‡é€‰ï¼Œæ™ºè°± GLM å®Œå…¨å…è´¹é€‚åˆå…¥é—¨ã€‚

**Q: å¤š API Key è½®è¯¢æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ**
A: 5 ä¸ª Key = 5 å€ååé‡ï¼å¯ä»¥å¤§å¹…ç¼©çŸ­å¤„ç†æ—¶é—´ã€‚å»ºè®®æ¯ä¸ªå¹³å°å‡†å¤‡ 3-5 ä¸ª Keyã€‚

**Q: API Key è¢«å°ç¦äº†æ€ä¹ˆåŠï¼Ÿ**
A: ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶å°†è¢«å°çš„ Key åŠ å…¥é»‘åå•ï¼Œä¸å½±å“å…¶ä»– Key ç»§ç»­å·¥ä½œã€‚

**Q: Is the Zhipu API really free?**
A: Yes! The GLM-4.6v-Flash model has a free tier with no limits. Perfect for getting started.

**Q: Can I use this for Korean/English books?**
A: Currently optimized for Japanese â†’ Chinese. Other languages may work but are untested.

**Q: How long does processing take?**
A: Depends on book size and API Key count. With 5 Keys, a typical 100k character book takes 3-8 minutes.

**Q: å¦‚ä½•æ·»åŠ æ–°çš„ LLM å¹³å°ï¼Ÿ**
A: åœ¨ `config.json` çš„ `platforms` æ•°ç»„ä¸­æ·»åŠ æ–°å¹³å°é…ç½®ï¼Œè®¾ç½® `activate_platform` ä¸ºæ–°å¹³å°çš„ IDã€‚

**Q: Why are some names not extracted?**
A: The NER model focuses on names with kana. Pure kanji names (like ç”°ä¸­) are skipped as they don't need transliteration.

**Q: æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆthinkingï¼‰æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ**
A: å¯ç”¨åæ¨¡å‹ä¼šè¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†åˆ†æï¼Œç¿»è¯‘è´¨é‡æ›´é«˜ï¼Œä½†é€Ÿåº¦ç•¥æ…¢ã€‚æ¨èä¿æŒå¼€å¯ã€‚

---

<p align="center">
  <strong>ğŸ“š BookTerm Gacha v0.2.0</strong>
  <br>
  <em>Multi-Platform LLM Support with API Key Rotation</em>
  <br>
  <em>Transforming Japanese Literatures into Translation-Ready Glossaries</em>
  <br><br>
  Made for the Japanese Literatures Translation Community
</p>
