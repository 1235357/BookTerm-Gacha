<h1><p align="center">ğŸ“š BookTerm Gacha</p></h1>
<p align="center"><strong>An LLM-Powered Agent for Automated Book Terminology Extraction</strong></p>


<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python"/>
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License"/>
  <img src="https://img.shields.io/badge/LLM-Agent-orange.svg" alt="LLM Agent"/>
  <img src="https://img.shields.io/badge/NER-BERT-purple.svg" alt="BERT NER"/>
</p>

---

## ğŸ¯ Project Overview

**BookTerm Gacha** is a specialized fork of [KeywordGacha v0.13.1](https://github.com/neavo/KeywordGacha), redesigned as an **LLM Agent** specifically optimized for:

- ğŸ“– **Book-focused terminology extraction** (EPUB, TXT, MD formats)
- ğŸ‡¯ğŸ‡µ **Japanese light novel optimization** with zero-tolerance for kana residue
- ğŸ¤– **LLM Agent development practice** - a real-world AI agent implementation
- ğŸ”§ **Customizable workflow** with transparent, debuggable stages

### What Makes This Different?

| Feature | Original KG v0.13.1 | BookTerm Gacha (This Project) |
|---------|---------------------|-------------------------------|
| **Focus** | General (games, subtitles, books) | Books only (EPUB, TXT, MD) |
| **Target Language** | Multi-language | Optimized for Japanese â†’ Chinese |
| **Kana Handling** | Basic detection | Strict detection + smart tolerance |
| **Retry Logic** | Simple retry | Staged retry + forced transliteration |
| **Progress Display** | Basic logging | Rich progress bars |
| **Result Validation** | None | Comprehensive result checker |
| **Agent Design** | Monolithic | Modular LLM Agent architecture |

---

## ğŸ§  Core Philosophy: LLM Agent Development

This project is a **practical LLM Agent development exercise**. The core insight is:

> **A terminology table is essentially a mapping from source language entities to target language translations.**
> 
> For Japanese books, this means: `æ—¥æ–‡å‡å/æ±‰å­— â†’ ä¸­æ–‡è¯‘å`

### The Agent Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BookTerm Gacha Workflow                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STAGE 1: NER Entity Extraction (BERT Model)                     â”‚  â”‚
â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚  â”‚
â”‚  â”‚  â€¢ Input: Raw book text (EPUB/TXT/MD)                            â”‚  â”‚
â”‚  â”‚  â€¢ Model: Fine-tuned BERT for Japanese NER                       â”‚  â”‚
â”‚  â”‚  â€¢ Output: Entity list (PER: persons, LOC: locations)            â”‚  â”‚
â”‚  â”‚  â€¢ Filter: Only keep entities WITH kana (pure kanji filtered)    â”‚  â”‚
â”‚  â”‚  â€¢ GPU: Auto-detect CUDA for acceleration                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STAGE 2: Context Translation (LLM)                              â”‚  â”‚
â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚  â”‚
â”‚  â”‚  â€¢ For each entity: Sample N context paragraphs                  â”‚  â”‚
â”‚  â”‚  â€¢ LLM translates context to Chinese                             â”‚  â”‚
â”‚  â”‚  â€¢ Validation: Check for degradation, kana residue               â”‚  â”‚
â”‚  â”‚  â€¢ Retry: Up to 8 times with context reduction strategy          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STAGE 3: Semantic Analysis (LLM)                                â”‚  â”‚
â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚  â”‚
â”‚  â”‚  â€¢ Input: Entity + Original context + Translated context         â”‚  â”‚
â”‚  â”‚  â€¢ LLM outputs: { summary, group, gender, translation }          â”‚  â”‚
â”‚  â”‚  â€¢ Strict validation: Zero kana in translation                   â”‚  â”‚
â”‚  â”‚  â€¢ Fallback: Forced transliteration after 5 retries              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  STAGE 4: Result Validation & Output                             â”‚  â”‚
â”‚  â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚  â”‚
â”‚  â”‚  â€¢ ResultChecker: Scan for kana residue, similarity issues       â”‚  â”‚
â”‚  â”‚  â€¢ Output: JSON glossary, log files, GalTransl format            â”‚  â”‚
â”‚  â”‚  â€¢ Report: Detailed statistics and issue tracking                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Technical Deep Dive

### Key Innovations

#### 1. Smart Kana Detection with Tolerance

```python
# Problem: Some kana should be tolerated (onomatopoeia, place name particles)
# Solution: Context-aware detection

RULE_ONOMATOPOEIA = frozenset({
    "ãƒƒ", "ã£",      # Sokuon (gemination)
    "ã", "ãƒ", "ã…", "ã‡", "ã‰",  # Small vowels
    "ã‚ƒ", "ã‚…", "ã‚‡", "ã‚",        # Small ya/yu/yo
    "ãƒ¼",            # Long vowel mark
    "ãƒ¶", "ã‚±", "ãƒµ", # Place name particles (å‰ãƒ¶æµœ â†’ å‰ä¹‹æ»¨)
    "ã®",            # Possessive particle in place names
})

# Only flag as "kana residue" if the kana is NOT isolated
# e.g., "å’–ãƒƒå•¡" â†’ tolerate (isolated ãƒƒ)
# e.g., "ã‚«ãƒƒã‚³ã„ã„" â†’ flag (ãƒƒ surrounded by kana)
```

#### 2. Staged Retry with Forced Transliteration

```python
MAX_RETRY = 8
FORCE_TRANSLITERATE_THRESHOLD = 5

# After 5 failed retries:
# 1. Use pykakasi to convert to romaji
# 2. Map romaji to Chinese phonetic equivalents
# 3. Guarantee a Chinese output (no kana residue)
```

#### 3. NER Filtering Strategy

```python
# Key insight: We only need entities WITH kana
# Pure kanji entities (ç”°ä¸­, æ±äº¬) don't need terminology tables
# They can be directly preserved or simply converted

def verify_by_language(text: str, language: int) -> bool:
    if language == Language.JA:
        # Must contain at least one kana character
        if not (any_hiragana(text) or any_katakana(text)):
            return False  # Filter out pure kanji
    return True
```

#### 4. Prompt Engineering for Particle Handling

```
ã€group Selection Rules (Important)ã€‘
- If it's a particle/auxiliary word (ã®ã€ã¯ã€ãŒã€ã‚’ã€ã§ã™ã€ã¾ã™, etc.)
  â†’ Must select "æ— æ³•åˆ¤æ–­" (Cannot Determine) or "å…¶ä»–" (Other)

ã€Special Kana Handling in Place Namesã€‘
- ãƒ¶ / ã‚± / ãƒµ: Means "of/no", e.g., ã€Œå‰ãƒ¶æµœã€â†’ã€Œå‰ä¹‹æ»¨ã€
- ã®: Means "of", e.g., ã€Œè¦‹æ™´ã‚‰ã—ã®ä¸˜ã€â†’ã€Œç­æœ›ä¹‹ä¸˜ã€
```

---

## ğŸ“¦ Installation & Setup

### Prerequisites

- **Python 3.10+** (3.11 or 3.12 recommended)
- **NVIDIA GPU** (optional but recommended for NER acceleration)
- **LLM API access** (free option available, see below)

### Step-by-Step Installation

#### Step 1: Clone or Download

```bash
# Option A: Clone with git
git clone https://github.com/YOUR_USERNAME/BookTermGacha.git
cd BookTermGacha

# Option B: Download ZIP and extract
```

#### Step 2: Create Virtual Environment (Recommended)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python -m venv venv
source venv/bin/activate
```

#### Step 3: Install PyTorch with CUDA (GPU Users)

**âš ï¸ IMPORTANT**: Install PyTorch FIRST with the correct CUDA version for GPU acceleration.

```bash
# Check your CUDA version first
nvidia-smi

# Then install PyTorch with matching CUDA version:

# CUDA 12.6 (Latest GPUs - RTX 40 series, etc.)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

# CUDA 12.4
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CUDA 11.8 (Older GPUs)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CPU Only (No NVIDIA GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

#### Step 4: Install Other Dependencies

```bash
pip install -r requirements.txt
```

#### Step 5: Verify Installation

```bash
# Check GPU availability
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\"}')"

# Check transformers
python -c "from transformers import AutoModel; print('Transformers OK')"
```

#### Step 6: Configure API

Edit `config.json` with your LLM API credentials (see below).

#### Step 7: Run

```bash
python app.py
```

### LLM API Configuration

Edit `config.json`:

```json
{
    "api_key": ["YOUR_API_KEY", "API key from your LLM provider"],
    "base_url": ["https://open.bigmodel.cn/api/paas/v4", "API endpoint URL"],
    "model_name": ["glm-4-flash", "Model name to use"]
}
```

#### ğŸ†“ Free API Option: Zhipu AI (æ™ºè°±AI / BigModel)

You can use **FREE** models from [bigmodel.cn](https://bigmodel.cn/):

1. Register at https://bigmodel.cn/
2. Get your API key from the console
3. Use these settings:
   ```json
   {
       "api_key": ["your-api-key-here"],
       "base_url": ["https://open.bigmodel.cn/api/paas/v4"],
       "model_name": ["glm-4-flash"]
   }
   ```

**Note**: `glm-4-flash` and `glm-4v-flash` are FREE models with generous rate limits!

#### Other Supported Providers

| Provider | Base URL | Recommended Model |
|----------|----------|-------------------|
| DeepSeek | `https://api.deepseek.com/v1` | `deepseek-chat` |
| OpenAI | `https://api.openai.com/v1` | `gpt-4o-mini` |
| Volcano Engine | See [wiki](https://github.com/neavo/KeywordGacha/wiki/VolcEngine) | `doubao-pro-32k` |

---

## ğŸ“ Input & Output

### Supported Input Formats

| Format | Extension | Description |
|--------|-----------|-------------|
| EPUB | `.epub` | E-book format (recommended for light novels) |
| Plain Text | `.txt` | UTF-8 encoded text files |
| Markdown | `.md` | Markdown documents |

Place your files in the `input/` folder before running.

### Output Files

After processing, you'll find these files in `output/`:

| File | Description |
|------|-------------|
| `input_è§’è‰²_è¯å…¸.json` | Character terminology dictionary |
| `input_è§’è‰²_æœ¯è¯­è¡¨.json` | LinguaGacha glossary format |
| `input_è§’è‰²_galtransl.txt` | GalTransl GPT dictionary format |
| `input_è§’è‰²_æ—¥å¿—.txt` | Detailed analysis log with summaries |
| `input_åœ°ç‚¹_*.json/txt` | Same formats for locations |
| `ç»“æœæ£€æŸ¥_æŠ¥å‘Š.json` | Quality check report |

### JSON Dictionary Format

```json
[
    {
        "src": "ã‚¢ãƒªã‚¹",
        "dst": "çˆ±ä¸½ä¸",
        "info": "å¥³ä¸»è§’ï¼Œé‡‘å‘ç¢§çœ¼çš„å°‘å¥³ï¼Œæ‹¥æœ‰æ²»æ„ˆé­”æ³•çš„èƒ½åŠ›ã€‚"
    },
    {
        "src": "ãƒˆãƒªã‚·ãƒ¥ãƒ¼ãƒ©",
        "dst": "ç‰¹é‡Œä¿®æ‹‰",
        "info": "ç¥ç§˜çš„å‰‘å£«ï¼ŒçœŸå®èº«ä»½ä¸æ˜ã€‚"
    }
]
```

### LinguaGacha Glossary Format

```json
[
    {
        "src": "ã‚¢ãƒªã‚¹",
        "dst": "çˆ±ä¸½ä¸",
        "info": "è§’è‰² - å¥³ - å¥³ä¸»è§’ï¼Œé‡‘å‘ç¢§çœ¼çš„å°‘å¥³..."
    }
]
```

---

## ğŸ”§ Configuration Options

Edit `config.json` to customize behavior:

| Option | Default | Description |
|--------|---------|-------------|
| `count_threshold` | `2` | Minimum occurrence count to include entity |
| `score_threshold` | `0.60` | NER confidence threshold (0.0-1.0) |
| `max_display_length` | `32` | Maximum entity display length |
| `max_context_samples` | `5` | Number of context paragraphs to sample |
| `tokens_per_sample` | `512` | Max tokens per context sample |
| `ner_target_types` | `["PER", "LOC"]` | Entity types to extract |
| `request_timeout` | `1800` | API request timeout (seconds) |
| `request_frequency_threshold` | `5` | Max requests per second |
| `traditional_chinese_enable` | `false` | Output Traditional Chinese |

---

## ğŸ—ï¸ Project Structure

```
BookTermGacha/
â”œâ”€â”€ app.py                  # Main entry point
â”œâ”€â”€ config.json             # Configuration file
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ version.txt             # Version info (v0.13.1-Refactor)
â”‚
â”œâ”€â”€ model/                  # Core models
â”‚   â”œâ”€â”€ LLM.py             # LLM agent with retry logic, validation
â”‚   â”œâ”€â”€ NER.py             # BERT-based NER extraction
â”‚   â””â”€â”€ Word.py            # Word data structure
â”‚
â”œâ”€â”€ module/                 # Utility modules
â”‚   â”œâ”€â”€ FileManager.py     # File I/O handling
â”‚   â”œâ”€â”€ LogHelper.py       # Logging utilities (Rich-based)
â”‚   â”œâ”€â”€ ResultChecker.py   # Quality validation & reporting
â”‚   â”œâ”€â”€ RubyCleaner.py     # Ruby/furigana annotation removal
â”‚   â”œâ”€â”€ Normalizer.py      # Text normalization
â”‚   â”œâ”€â”€ File/              # Format-specific readers
â”‚   â”‚   â”œâ”€â”€ EPUB.py        # EPUB reader
â”‚   â”‚   â”œâ”€â”€ TXT.py         # TXT reader
â”‚   â”‚   â””â”€â”€ MD.py          # Markdown reader
â”‚   â””â”€â”€ Text/              # Text processing utilities
â”‚       â”œâ”€â”€ TextHelper.py  # Character detection, manipulation
â”‚       â””â”€â”€ TextBase.py    # Base text utilities
â”‚
â”œâ”€â”€ prompt/                 # LLM prompts (customizable)
â”‚   â”œâ”€â”€ prompt_context_translate.txt
â”‚   â”œâ”€â”€ prompt_surface_analysis_with_context.txt
â”‚   â”œâ”€â”€ prompt_surface_analysis_with_translation.txt
â”‚   â””â”€â”€ prompt_surface_analysis_without_translation.txt
â”‚
â”œâ”€â”€ blacklist/             # Filter lists
â”‚   â”œâ”€â”€ jp_è¯­æ°”åŠ©è¯.json   # Japanese particles blacklist
â”‚   â”œâ”€â”€ jp_äººç§°ä»£è¯.json   # Japanese pronouns blacklist
â”‚   â”œâ”€â”€ jp_äº²å±å…³ç³».json   # Japanese family terms blacklist
â”‚   â””â”€â”€ custom.json        # Custom blacklist (add your own)
â”‚
â”œâ”€â”€ resource/              # Resources
â”‚   â”œâ”€â”€ kg_ner_bf16/       # BERT NER model (required)
â”‚   â””â”€â”€ llm_config/        # LLM configuration presets
â”‚
â”œâ”€â”€ input/                 # Place your books here
â”œâ”€â”€ output/                # Generated terminology tables
â””â”€â”€ docs/                  # Documentation
    â””â”€â”€ IMPROVEMENT_ANALYSIS.md  # Technical analysis
```

---

## ğŸ”„ Comparison with Original KeywordGacha

### Architecture Comparison

| Aspect | KG v0.13.1 (Original) | KG v0.20.2 (New) | BookTerm Gacha (This) |
|--------|----------------------|------------------|----------------------|
| **UI** | CLI | GUI (PyQt) | CLI (Rich) |
| **NER** | BERT | Native LLM | BERT + Smart Filter |
| **Focus** | General | General | Books only |
| **Workflow** | 2-stage | AI-native | 4-stage Agent |
| **Validation** | None | Basic | Comprehensive |
| **Kana Handling** | Basic | Basic | Strict + Tolerance |
| **Fallback** | None | None | Forced Transliteration |

### What We Borrowed from KG v0.13.1

- âœ… BERT NER model and tokenization pipeline
- âœ… Basic workflow structure (NER â†’ Context â†’ Analysis)
- âœ… File format readers (EPUB, TXT, MD)
- âœ… Blacklist filtering system

### What We Borrowed from LinguaGacha (Dev-Experimental)

- âœ… `ResponseChecker` patterns (degradation detection)
- âœ… `TextHelper` precise character set definitions
- âœ… `KanaFixer` onomatopoeia handling logic
- âœ… Kana tolerance ratio concept (10%)

### Our Innovations

1. **Smart NER Filtering**: Only keep kana-containing entities (pure kanji filtered)
2. **Staged Retry with Fallback**: Guaranteed Chinese output via forced transliteration
3. **Place Name Particle Handling**: ãƒ¶, ã®, etc. treated as "ä¹‹"
4. **Prompt Engineering**: Guide LLM to handle particles and edge cases
5. **Rich Progress Display**: Clear visibility into agent operations
6. **Comprehensive Validation**: Detect and report all quality issues
7. **Result Checker**: Post-processing quality assurance

---

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest test_improvements.py -v

# Test specific functionality
python -m pytest test_improvements.py::test_contains_kana_strict -v
python -m pytest test_improvements.py::test_result_checker -v
```

### Test Coverage

- âœ… `test_contains_kana_strict` - Kana detection with tolerance
- âœ… `test_is_degraded` - Degradation detection (repeated characters)
- âœ… `test_check_similarity` - Jaccard similarity checking
- âœ… `test_force_transliterate` - Forced transliteration fallback
- âœ… `test_verify_kana_only` - NER filtering logic
- âœ… `test_result_checker` - Result validation module
- âœ… `test_blacklist_particles` - Particle blacklist filtering

---

## ğŸš€ Building for Release

### Development Setup

```bash
# Install development dependencies
pip install -r requirements.txt
pip install pyinstaller pytest

# Run tests
python -m pytest test_improvements.py -v

# Check syntax
python -m py_compile app.py model/LLM.py model/NER.py
```

### Creating Executable

```bash
# Build with PyInstaller
pyinstaller --onefile --name BookTermGacha app.py

# The executable will be in dist/BookTermGacha.exe
```

### Release Package Structure

```
BookTermGacha-v0.13.1-Refactor/
â”œâ”€â”€ BookTermGacha.exe      # Main executable (or app.py for source)
â”œâ”€â”€ config.json            # Configuration (user edits this)
â”œâ”€â”€ requirements.txt       # For source installations
â”œâ”€â”€ README.md              # This file
â”‚
â”œâ”€â”€ prompt/                # LLM prompts
â”‚   â””â”€â”€ *.txt
â”‚
â”œâ”€â”€ blacklist/             # Filter lists
â”‚   â””â”€â”€ *.json
â”‚
â”œâ”€â”€ resource/
â”‚   â””â”€â”€ kg_ner_bf16/       # BERT model (REQUIRED - ~500MB)
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ input/                 # User places books here
â”‚   â””â”€â”€ (empty)
â”‚
â””â”€â”€ output/                # Results appear here
    â””â”€â”€ (empty)
```

### GPU Support

The NER stage automatically detects CUDA:
- **With GPU**: Uses bf16 precision for fast inference
- **Without GPU**: Falls back to CPU (slower but works)

No configuration needed - it's automatic!

---

## ğŸ“‹ Changelog

### v0.13.1-Refactor (Current)

**New Features:**
- Smart kana detection with onomatopoeia tolerance
- Place name particle handling (ãƒ¶, ã‚±, ãƒµ, ã®)
- Forced transliteration fallback (romaji â†’ Chinese)
- Rich progress bars for all stages
- Comprehensive ResultChecker module
- Particle handling in prompts

**Improvements:**
- Reduced max retry: 32 â†’ 8
- Earlier forced transliteration: after 5 retries
- Simplified logging output
- Better error messages

**Based on:**
- KeywordGacha v0.13.1 (core workflow)
- LinguaGacha Dev-Experimental (validation patterns)

---

## ğŸ™ Acknowledgments

- [KeywordGacha](https://github.com/neavo/KeywordGacha) by neavo - Original project and inspiration
- [LinguaGacha](https://github.com/neavo/LinguaGacha) by neavo - Design patterns and utilities
- [Zhipu AI / BigModel](https://bigmodel.cn/) - Free LLM API for development and testing

---

## ğŸ“„ License

This project is based on KeywordGacha and follows the same licensing terms.

**Important**: If you use this tool in your translation work, please credit appropriately.

---

## ğŸ¤ Contributing

This project serves as an **LLM Agent development learning exercise**. Contributions are welcome!

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes
4. Run tests: `python -m pytest test_improvements.py -v`
5. Commit: `git commit -m 'Add amazing feature'`
6. Push: `git push origin feature/amazing-feature`
7. Open a Pull Request

### Areas for Contribution

- ğŸŒ Multi-language support (Korean, English sources)
- ğŸ“Š Better progress visualization
- ğŸ”§ Additional output formats
- ğŸ“ Documentation improvements
- ğŸ§ª More test cases

---

## â“ FAQ

**Q: Why focus only on books?**
A: Games and subtitles have different terminology patterns. Books (especially light novels) have consistent character/location naming that benefits most from terminology tables.

**Q: Why filter out pure kanji entities?**
A: Pure kanji entities (ç”°ä¸­, æ±äº¬) don't need terminology tables - they can be preserved as-is or trivially converted. The real challenge is kana (ã‚¢ãƒªã‚¹, ãƒˆãƒªã‚·ãƒ¥ãƒ¼ãƒ©) which need proper transliteration.

**Q: Why use BERT + LLM instead of pure LLM?**
A: BERT NER is faster and more reliable for entity extraction. LLM is better for semantic analysis and translation. Combining both gives the best results.

**Q: Can I use other LLM providers?**
A: Yes! Any OpenAI-compatible API works. Just update `config.json` with your provider's URL and API key.

---

<p align="center">
  <strong>Built with â¤ï¸ as an LLM Agent Development Exercise</strong>
  <br>
  <em>Transforming Japanese Literatures into Translation-Ready Glossaries</em>
</p>
