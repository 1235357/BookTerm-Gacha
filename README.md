# BookTerm Gacha

An LLM-powered terminology extraction agent for books and story scripts.

**Version:** `v0.2.0-Multi-Platform-API-Rotation`

BookTerm Gacha reads your source text (EPUB/TXT/MD and more), extracts named entities with a local BERT NER model, then uses an OpenAI-compatible LLM API to **classify** and **translate / transliterate** the terms into a glossary you can feed into translation tools.

This fork is heavily optimized for **Japanese â†’ Chinese** name/place consistency (katakana handling, kana-residue checks, forced transliteration fallback), while still supporting **ZH/EN/JA/KO** input modes.

---

## What you get

For each detected term group, the tool generates:

- `output/<input>_<group>_è¯å…¸.json` â€” simple dictionary mapping (`src -> dst`)
- `output/<input>_<group>_æœ¯è¯­è¡¨.json` â€” glossary list with metadata (`src/dst/info`)
- `output/<input>_<group>_galtransl.txt` â€” TSV for GalTransl-style import
- `output/<input>_<group>_æ—¥å¿—.txt` â€” human-readable audit log (context, translation, summary)

And a post-run QA pass produces reports like:

- `output/ç»“æœæ£€æŸ¥_å‡åæ®‹ç•™.json`
- `output/ç»“æœæ£€æŸ¥_ç›¸ä¼¼åº¦è¿‡é«˜.json`
- `output/ç»“æœæ£€æŸ¥_ç©ºç¿»è¯‘.json`
- `output/ç»“æœæ£€æŸ¥_æŠ¥å‘Š.json`

> Note: Output filenames currently use Chinese group names (`è§’è‰²`, `åœ°ç‚¹`) because the pipelineâ€™s classification taxonomy is Chinese-first.

---

## Project layout

Key folders/files in this workspace:

```
BookTerm Gacha/
  app.py                     # main entry (menu + pipeline)
  config.json                 # runtime settings (multi-platform, thresholds)
  version.txt

  input/                      # put your books/scripts here
  output/                     # generated glossary + QA reports
  log/                        # runtime logs

  model/
    NER.py                    # BERT NER extraction
    LLM.py                    # LLM agent (streaming, retry, rotation)
    Word.py                   # term object + context sampling

  module/
    FileManager.py            # reads many formats; writes outputs
    EnvChecker.py             # auto environment check/repair (source run)
    ResultChecker.py          # QA checks
    RubyCleaner.py            # removes Japanese ruby/furigana
    TaskTracker.py            # persistent bottom progress panel

  prompt/                     # LLM prompt templates (Chinese)
  blacklist/                  # language-specific stopword lists

  resource/
    kg_ner_bf16/              # local NER model (token-classification)
    llm_config/               # per-stage LLM generation configs
```

---

## Quick start (Windows)

### Option A: Use the packaged EXE

If you have a built release (`app.exe`), you typically only need to:

1. Edit `config.json` (set platform + API keys).
2. Put your files into `input/`.
3. Run `app.exe`.

> In PyInstaller builds, environment checks are skipped because dependencies are bundled.

### Option B: Run from source (recommended for developers / GPU)

Requirements:

- Python **3.10+**
- (Optional) NVIDIA GPU for faster NER

Run:

- `00_å¼€å‘.bat` (Windows helper), or
- `python app.py`

On first run, `app.py` calls `module/EnvChecker.py` to:

- validate Python version
- detect GPU/CUDA
- install missing packages using common mirrors
- reinstall PyTorch with CUDA wheels when a GPU is present but PyTorch is CPU-only

---

## Configuration (config.json)

`config.json` is **the main runtime configuration**.

This project uses a â€œvalue + commentâ€ style for many fields:

```json
"count_threshold": [2, "...comment..." ]
```

Only the **first** element is read as the real value.

### 1) Multi-platform LLM configuration

BookTerm Gacha talks to LLMs via **OpenAI-compatible Chat Completions** (`openai.AsyncOpenAI`), and can switch between multiple platforms.

Key fields:

- `activate_platform`: the `id` of the active platform entry.
- `platforms[]`: list of platform configs.
  - `api_url`: base URL (OpenAI-compatible)
  - `api_key`: **array** of keys (supports rotation)
  - `model`: model name string
  - `thinking`: hint flag (platform-specific; see below)

Example (trimmed):

```json
{
  "activate_platform": 1,
  "platforms": [
    {
      "id": 1,
      "name": "NVIDIA-DeepSeek-V3.2",
      "api_url": "https://integrate.api.nvidia.com/v1",
      "api_key": ["key1", "key2"],
      "model": "deepseek-ai/deepseek-v3.2",
      "thinking": true
    }
  ]
}
```

### 2) API key rotation & blacklist

If `api_key` contains multiple keys, the tool enables **round-robin rotation**:

- Each request picks the next available key.
- If a key hits a **provider ban / blacklist** (detected from `PermissionDenied` + keywords like `banned/suspended/blacklist`), the key is added to an in-memory blacklist and skipped.
- If all keys are blacklisted, the run stops with a fatal error.

This is designed for high-throughput batch runs where you want to:

- avoid per-key rate ceilings
- reduce total wall-clock time by distributing load

### 3) Rate limiting & concurrency

Two knobs control request pressure:

- `request_frequency_threshold`: â€œhow many requests per second we *send*â€
- `max_concurrent_requests`: â€œhow many requests can be *in-flight* simultaneouslyâ€

Implementation details (see `model/LLM.py`):

- A semaphore caps concurrent in-flight requests.
- An async rate limiter (`aiolimiter.AsyncLimiter`) caps the send rate.

Why two limits? Because your average request latency matters:

If average latency is $L$ seconds, and you send $R$ requests/sec, youâ€™ll have about $R \cdot L$ requests in flight. A high $L$ with a high $R$ will explode concurrency unless capped.

Practical tuning tips:

- Getting `429 Too Many Requests`:
  - lower `request_frequency_threshold`
  - and/or lower `max_concurrent_requests`
- You have many keys and want speed:
  - keep `request_frequency_threshold` around `5â€“10`
  - set `max_concurrent_requests` higher (e.g. `30â€“90`) *only if your provider and network can handle it*

### 4) NER and filtering knobs

These parameters are crucial to keep LLM calls affordable and improve quality.

- `score_threshold` (default `0.60`)
  - After NER, terms below this confidence are dropped.
- `count_threshold` (default `2`)
  - Terms with fewer occurrences are dropped.
  - This saves a lot of LLM tokens on one-off noise.
- `max_display_length` (default `32`)
  - Filters out abnormal long â€œtermsâ€.
- `ner_target_types` (default `['PER', 'LOC']`)
  - NER stage keeps only selected entity types.
  - Including `ORG/PRD` increases coverage but can multiply LLM cost.

### 5) Context sampling controls

LLMs are only as good as the context you give them. This project implements **diverse context sampling** in `model/Word.py`:

- `max_context_samples` (default `5` in config; loaded into `Word.MAX_CONTEXT_SAMPLES`)
- `tokens_per_sample` (default `512` in config; loaded into `Word.TOKENS_PER_SAMPLE`)

How it works:

1. For each term, all lines containing the term are collected.
2. The occurrences are split into $N$ evenly-spaced groups.
3. From each groupâ€™s starting position, adjacent lines are aggregated until ~80% of the token budget.
4. Each aggregated paragraph is emitted with a stable header `ã€ä¸Šä¸‹æ–‡ iã€‘`.

This yields contexts from different parts of the book (first appearance, later usage, dialogue vs narration, etc.).

### 6) Traditional Chinese output

- `traditional_chinese_enable`: if `true`, output Chinese strings are converted using OpenCC (`s2tw`).

---

## How the pipeline works (v0.2.0)

The runtime workflow lives in `app.py`.

### Stage 0 â€” Text ingestion & normalization

`module/FileManager.py` reads files from `input/` (or a user-provided path) and normalizes lines:

- Unicode normalization + fullwidth/halfwidth fixes (`module/Normalizer.py`)
- whitespace cleanup
- language filtering (keeps only lines containing characters of the selected language)

Supported input formats include:

- books: `.epub`, `.txt`, `.md`, `.xlsx`
- scripts/subtitles: `.ass`, `.srt`, `.rpy`, `.trans`
- json formats: KV-style JSON, message JSON

### Stage 1 â€” NER (local BERT, GPU-friendly)

`model/NER.py` uses Hugging Face Transformers `pipeline('token-classification')` with aggregation.

Key techniques:

- **Chunking to 512 tokens** so long books can be processed safely.
- **bf16/fp16** on GPU when available (`is_torch_bf16_gpu_available`).
- **Ruby/Furigana removal** (`module/RubyCleaner.py`) for Japanese sources.
- **Stopword blacklist** (`blacklist/*.json`) to remove pronouns/particles/etc.
- **Japanese-specific rule**: keep only candidates that contain kana (hiragana/katakana), because the main use case is katakana-style names that must be transliterated.

The result is a list of candidate entities with confidence scores.

### Stage 1.5 â€” Heuristic filtering

In `app.py`:

- merge duplicates by surface, keep max score
- drop low-confidence by `score_threshold`
- search per-term contexts; compute occurrence count
- drop low-frequency by `count_threshold`
- drop overly long â€œtermsâ€ by `max_display_length`

These filters reduce LLM workload dramatically.

### Stage 2 â€” LLM context translation (NEW workflow)

For non-Chinese source languages, v0.2.0 forces a two-step strategy:

1. **Translate sampled contexts into Simplified Chinese** (`context_translate_batch`).
2. Use the translated context to improve the next stageâ€™s semantic decisions.

Hard rules in the translation prompt (see `prompt/prompt_context_translate.txt`):

- keep `ã€ä¸Šä¸‹æ–‡ Nã€‘` headers exactly
- keep line alignment (no merging/splitting lines)
- forbid copying kana/latin/hangeul into the Chinese output

Robustness tactics:

- per-task timeout with `asyncio.wait_for`
- automatic context shrinking when timeouts or content filters happen

### Stage 3 â€” LLM surface analysis (classification + final term translation)

For each term, the LLM returns a JSON object with:

- `summary` (Chinese)
- `group` (fine-grained taxonomy)
- `gender` (`ç”·/å¥³/æ— æ³•åˆ¤æ–­`)
- `translation` (final Chinese term)

Then the code maps fine-grained types into output groups:

- `è§’è‰²` (Character) â€” includes `å§“æ°/åå­—/...`
- `åœ°ç‚¹` (Location)

Everything else is filtered out as â€œnot targetâ€.

Quality gates (automatic retries):

- strict kana residue detection (allows isolated onomatopoeia markers like `ãƒƒ`, `ãƒ¶`)
- hangeul residue detection for KO
- degraded output detection (repetitive patterns)
- â€œtranslation failedâ€ similarity detection (Jaccard similarity â‰¥ 0.80)

### Stage 4 â€” Automatic fix pass (problem terms only)

After surface analysis, a third-stage fixer (`fix_translation_batch`) runs only on problematic entries:

- empty translations
- kana residue
- similarity-too-high

If fixing still fails, the tool uses a **forced transliteration fallback**:

1. Convert to romaji via `pykakasi`
2. Map romaji syllables to common Chinese phonetic chunks

This ensures you always get a usable Chinese output, even under bad model behavior.

### Stage 5 â€” Output & QA reports

Finally, outputs are written per group, then `module/ResultChecker.py` generates QA reports.

---

## LLM platform compatibility notes

`model/LLM.py` automatically detects certain platforms and adjusts request shape:

- Zhipu (bigmodel.cn): stream + thinking
- NVIDIA Build DeepSeek: stream + `chat_template_kwargs.thinking`
- ModelScope DeepSeek: stream + `enable_thinking`
- OpenAI O-series: uses `max_completion_tokens` instead of `max_tokens`

Everything else uses the standard OpenAI-compatible format.

LLM generation parameters are primarily controlled by:

- `resource/llm_config/context_translate_config.json`
- `resource/llm_config/surface_analysis_config.json`
- `resource/llm_config/api_test_config.json`

> The per-platform `temperature/top_p/...` fields in `config.json` are currently not the primary source of sampling parameters; they are reserved / informational.

---

## Troubleshooting

### â€œNo input foundâ€

- Put files into `input/`, or paste a path when prompted.

### GPU is not used

- NER uses GPU only if `torch.cuda.is_available()` is `True`.
- If you have an NVIDIA GPU but PyTorch is CPU-only, the environment checker will try to reinstall CUDA wheels.

### Too many requests / 429

- Lower `request_frequency_threshold`.
- Lower `max_concurrent_requests`.
- Add more API keys and keep rotation enabled.

### Timeouts

- Increase `request_timeout`.
- Increase `task_timeout_threshold` (per term; triggers context shrinking on timeout).

### Content filter errors

The tool automatically:

- excludes the sampled context indices that triggered filtering
- shrinks context sampling and retries

If it still fails, consider lowering:

- `max_context_samples`
- `tokens_per_sample`

### â€œAll API keys are blacklistedâ€

- Your provider rejected the keys (ban/suspension) and the tool stopped using them.
- Replace keys, or switch `activate_platform` to another provider.

### Output looks untranslated / too similar

- Check `output/ç»“æœæ£€æŸ¥_ç›¸ä¼¼åº¦è¿‡é«˜.json`.
- Try a stronger model, or lower temperature in `resource/llm_config/*`.

---

## Developer notes (build)

Windows helpers:

- `00_å¼€å‘.bat` â€” run from source
- `01_æ‰“åŒ….bat` â€” PyInstaller build

PyInstaller spec:

- `BookTermGacha.spec` bundles Python deps into `dist/BookTermGacha/_internal`
- `build_finish.py` copies runtime resources (prompts, blacklist, NER model, LLM configs)

---

## Credits

- Based on **KeywordGacha v0.13.1** (neavo) design and ecosystem.
- Bundled NER model under `resource/kg_ner_bf16/` declares **Apache-2.0** in its model card.

---

## About this README

This README is generated from the current workspace code in `BookTerm Gacha/` (v0.2.0) and focuses on the actual implemented behavior:

- three-stage LLM flow (translate â†’ analyze â†’ fix)
- multi-platform OpenAI-compatible APIs
- API key rotation + blacklist
- diversified context sampling
- strict QA checks and forced transliteration fallback
<h1><p align="center">ğŸ“š BookTerm Gacha</p></h1>
<p align="center"><strong>An LLM-Powered Agent for Automated Book Terminology Extraction</strong></p>
<p align="center"><em>Multi-Platform LLM Support with API Key Rotation - Extract Character & Location Names from Japanese Literatures</em></p>

<p align="center">
  <img src="https://img.shields.io/badge/Version-0.2.0-brightgreen.svg" alt="Version"/>
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python"/>
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License"/>
  <img src="https://img.shields.io/badge/LLM-Multi_Platform-orange.svg" alt="Multi Platform"/>
  <img src="https://img.shields.io/badge/NVIDIA-DeepSeek_V3.2-76B900.svg" alt="NVIDIA"/>
  <img src="https://img.shields.io/badge/ModelScope-é˜¿é‡Œäº‘ç™¾ç‚¼-blue.svg" alt="ModelScope"/>
  <img src="https://img.shields.io/badge/Zhipu-GLM_4.6v-red.svg" alt="Zhipu GLM"/>
  <img src="https://img.shields.io/badge/NER-BERT-purple.svg" alt="BERT NER"/>
  <img src="https://img.shields.io/badge/GPU-CUDA_Supported-76B900.svg" alt="CUDA"/>
</p>

---

## ğŸ“‹ Table of Contents

- [What is BookTerm Gacha?](#-what-is-bookterm-gacha)
- [Key Features](#-key-features)
- [Quick Start Guide](#-quick-start-guide)
- [Installation Methods](#-installation-methods)
  - [Method 1: Download Release (Recommended for Most Users)](#method-1-download-release-recommended-for-most-users)
  - [Method 2: Clone Repository (For Developers & GPU Users)](#method-2-clone-repository-for-developers--gpu-users)
- [Configuration Guide](#-configuration-guide)
- [How to Use](#-how-to-use)
- [Understanding the Output](#-understanding-the-output)
- [Troubleshooting](#-troubleshooting)
- [Technical Details](#-technical-details)
- [Release Notes](#-release-notes)
- [Acknowledgments](#-acknowledgments)
- [License](#-license)

---

## ğŸ¯ What is BookTerm Gacha?

**BookTerm Gacha** is an intelligent tool that automatically extracts character names, location names, and other important terminology from books and generates translation glossaries.

### The Problem It Solves

When translating Japanese Literatures to Chinese, translators face a common challenge:

> **How do you consistently translate character names like `ã‚¢ãƒªã‚¹`, `ãƒˆãƒªã‚·ãƒ¥ãƒ¼ãƒ©`, or `ãƒ†ã‚£ãƒŠãƒ¼ã‚·ãƒ£`?**

These katakana names need to be transliterated into Chinese, and keeping them consistent throughout a book (or book series) is tedious and error-prone.

### The Solution

BookTerm Gacha automates this process:

1. **Reads** your EPUB/TXT/MD book files
2. **Extracts** all character and location names using AI (BERT NER model)
3. **Analyzes** each name with context using LLM (Zhipu GLM)
4. **Generates** a terminology glossary ready for use with translation tools

### Who Is This For?

- ğŸ“– **Japanese Literatures Translators** - Create consistent terminology tables for your translation projects
- ğŸ® **Fan Translation Groups** - Standardize character names across team members
- ğŸ“š **Translation Tool Users** - Generate glossaries for [LinguaGacha](https://github.com/neavo/LinguaGacha), [GalTransl](https://github.com/xd2333/GalTransl), and similar tools
- ğŸ¤– **AI/LLM Enthusiasts** - Learn how to build practical LLM Agent applications

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ“– **Book-Focused** | Optimized specifically for EPUB, TXT, and MD book formats |
| ğŸ‡¯ğŸ‡µ **Japanese Optimized** | Fine-tuned for Japanese â†’ Chinese translation with zero kana residue |
| ï¿½ **Multi-Platform LLM** | Support for NVIDIA Build, ModelScope (é˜¿é‡Œäº‘ç™¾ç‚¼), Zhipu GLM, and more |
| ğŸ”„ **API Key Rotation** | Multiple API keys with automatic round-robin polling for massive throughput |
| ğŸš« **Smart Blacklist** | Automatic detection and blacklisting of banned/expired API keys |
| ğŸ†“ **Free Tier Options** | Zhipu GLM-4-Flash FREE, NVIDIA/ModelScope generous free quotas |
| ğŸš€ **GPU Acceleration** | Automatic CUDA detection for fast NER processing (optional) |
| ğŸ’¡ **Deep Thinking** | Support for reasoning models (DeepSeek R1, GLM with thinking) |
| ğŸ“Š **Rich Progress** | Beautiful progress bars showing exactly what's happening |
| âœ… **Quality Validation** | Automatic result checking for kana residue and issues |
| ğŸ“ **Multiple Formats** | Outputs in JSON dictionary, LinguaGacha, and GalTransl formats |

---

## ğŸš€ Quick Start Guide

**For users who just want to get started quickly:**

1. Download the latest release from [GitHub Releases](https://github.com/1235357/BookTerm-Gacha/releases)
2. Extract the ZIP file to any folder
3. Choose your LLM platform and get API key(s):
   - **NVIDIA Build** (Recommended): [build.nvidia.com](https://build.nvidia.com/) - DeepSeek V3.2 å…è´¹é¢åº¦
   - **ModelScope é˜¿é‡Œäº‘ç™¾ç‚¼**: [modelscope.cn](https://www.modelscope.cn/) - å…è´¹é¢åº¦
   - **Zhipu AI æ™ºè°±**: [bigmodel.cn](https://bigmodel.cn/) - GLM-4-Flash å®Œå…¨å…è´¹
4. Edit `config.json` - add your API keys and select platform
5. Put your EPUB/TXT files in the `input/` folder
6. Run `app.exe`
7. Find your terminology glossary in the `output/` folder

**That's it!** For detailed instructions, continue reading below.

---

## ğŸ“¦ Installation Methods

There are **two ways** to use BookTerm Gacha. Choose the one that fits your needs:

| Method | Best For | GPU Support | Difficulty |
|--------|----------|-------------|------------|
| **[Method 1: Download Release](#method-1-download-release-recommended-for-most-users)** | Most users, quick setup | CPU only (bundled) | â­ Easy |
| **[Method 2: Clone Repository](#method-2-clone-repository-for-developers--gpu-users)** | Developers, GPU users | Full CUDA support | â­â­â­ Advanced |

---

### Method 1: Download Release (Recommended for Most Users)

This is the **easiest way** to get started. The release package includes everything you need.

#### Step 1: Download the Release

1. Go to [GitHub Releases](https://github.com/1235357/BookTermGacha/releases)
2. Download the latest `BookTermGacha-v0.1.0.zip` file
3. Extract the ZIP to any folder (e.g., `C:\BookTermGacha\` or `D:\Tools\BookTermGacha\`)

#### Step 2: Understand the Folder Structure

After extraction, you'll see:

```
BookTermGacha/
â”œâ”€â”€ app.exe                 # Main executable - double-click to run
â”œâ”€â”€ config.json             # Configuration file - YOU NEED TO EDIT THIS
â”œâ”€â”€ version.txt             # Version information
â”‚
â”œâ”€â”€ blacklist/              # Filter lists (pre-configured, don't modify)
â”‚   â”œâ”€â”€ jp_è¯­æ°”åŠ©è¯.json
â”‚   â”œâ”€â”€ jp_äººç§°ä»£è¯.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ prompt/                 # LLM prompts (pre-configured, don't modify)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ resource/               # Required resources
â”‚   â”œâ”€â”€ kg_ner_bf16/        # BERT NER model (DO NOT DELETE)
â”‚   â””â”€â”€ llm_config/         # LLM configuration presets
â”‚
â”œâ”€â”€ input/                  # PUT YOUR BOOKS HERE
â”‚   â””â”€â”€ (empty - add your EPUB/TXT files)
â”‚
â”œâ”€â”€ output/                 # RESULTS APPEAR HERE
â”‚   â””â”€â”€ (empty - generated files will be here)
â”‚
â””â”€â”€ log/                    # Log files for troubleshooting
    â””â”€â”€ (auto-generated)
```

#### Step 3: Get Your API Keys

BookTerm Gacha æ”¯æŒå¤šä¸ª LLM å¹³å°ï¼Œå¯ä»¥é…ç½®å¤šä¸ª API Key è¿›è¡Œè½®è¯¢ä»¥è·å¾—æ›´é«˜ååé‡ï¼š

**ğŸ† æ¨èå¹³å°å¯¹æ¯”:**

| å¹³å° | æ¨¡å‹ | å…è´¹é¢åº¦ | é€Ÿåº¦ | è´¨é‡ | æ¨èåº¦ |
|------|------|----------|------|------|--------|
| **NVIDIA Build** | DeepSeek V3.2 | 1000 æ¬¡/å¤© | â­â­â­â­â­ | â­â­â­â­â­ | ğŸ¥‡ é¦–é€‰ |
| **ModelScope é˜¿é‡Œäº‘ç™¾ç‚¼** | DeepSeek V3.2/R1 | å……è¶³ | â­â­â­â­ | â­â­â­â­â­ | ğŸ¥ˆ å¤‡é€‰ |
| **Zhipu AI æ™ºè°±** | GLM-4.6v-Flash | æ— é™åˆ¶ | â­â­â­ | â­â­â­â­ | ğŸ¥‰ å…è´¹é¦–é€‰ |

**è·å– API Key:**

**NVIDIA Build (æ¨è):**
1. è®¿é—® [https://build.nvidia.com/](https://build.nvidia.com/)
2. æ³¨å†Œ/ç™»å½• NVIDIA è´¦å·
3. æœç´¢ "DeepSeek V3.2" æ¨¡å‹
4. ç‚¹å‡» "Get API Key" è·å–å¯†é’¥
5. å¯æ³¨å†Œå¤šä¸ªè´¦å·è·å–å¤šä¸ª Key ç”¨äºè½®è¯¢

**ModelScope é˜¿é‡Œäº‘ç™¾ç‚¼:**
1. è®¿é—® [https://www.modelscope.cn/](https://www.modelscope.cn/)
2. ä½¿ç”¨æ”¯ä»˜å®/é˜¿é‡Œäº‘è´¦å·ç™»å½•
3. è¿›å…¥æ¨¡å‹æ¨ç†é¡µé¢
4. è·å– API Key (æ ¼å¼: `ms-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`)

**Zhipu AI æ™ºè°± (å®Œå…¨å…è´¹):**
1. è®¿é—® [https://bigmodel.cn/](https://bigmodel.cn/)
2. æ³¨å†Œè´¦å· (éœ€æ‰‹æœºéªŒè¯)
3. è¿›å…¥æ§åˆ¶å°åˆ›å»º API Key

> ğŸ’¡ **æ€§èƒ½æç¤º**: ä½¿ç”¨å¤šä¸ª API Key è½®è¯¢å¯ä»¥å¤§å¹…æå‡å¤„ç†é€Ÿåº¦ï¼å»ºè®®æ¯ä¸ªå¹³å°å‡†å¤‡ 3-5 ä¸ª Keyã€‚

#### Step 4: Configure Your API Keys

1. Open `config.json` with any text editor (Notepad, VS Code, etc.)
2. The new multi-platform configuration format:

```json
{
    "activate_platform": 1,
    "platforms": [
        {
            "id": 0,
            "name": "æ™ºè°±GLM-4.6v-flash(å…è´¹)",
            "api_url": "https://open.bigmodel.cn/api/paas/v4",
            "api_key": ["your-zhipu-api-key"],
            "model": "glm-4.6v-flash",
            "thinking": true,
            "description": "æ™ºè°±å…è´¹æ¨¡å‹ï¼Œæ”¯æŒæ·±åº¦æ€è€ƒ"
        },
        {
            "id": 1,
            "name": "NVIDIA-DeepSeek-V3.2",
            "api_url": "https://integrate.api.nvidia.com/v1",
            "api_key": [
                "nvapi-key1",
                "nvapi-key2",
                "nvapi-key3"
            ],
            "model": "deepseek-ai/deepseek-v3.2",
            "thinking": true,
            "description": "NVIDIA Build DeepSeek V3.2ï¼Œå¤šKeyè½®è¯¢"
        },
        {
            "id": 2,
            "name": "é­”å¡”-DeepSeek-V3.2",
            "api_url": "https://api-inference.modelscope.cn/v1/",
            "api_key": [
                "ms-key1",
                "ms-key2"
            ],
            "model": "deepseek-ai/DeepSeek-V3.2",
            "thinking": true,
            "description": "é˜¿é‡Œäº‘ç™¾ç‚¼ ModelScope"
        }
    ]
}
```

3. **è®¾ç½® `activate_platform`** ä¸ºä½ æƒ³ä½¿ç”¨çš„å¹³å° ID:
   - `0` = æ™ºè°± GLM (å…è´¹)
   - `1` = NVIDIA DeepSeek V3.2 (æ¨è)
   - `2` = ModelScope DeepSeek V3.2
   - `3` = ModelScope DeepSeek R1

4. **æ·»åŠ ä½ çš„ API Keys** åˆ°å¯¹åº”å¹³å°çš„ `api_key` æ•°ç»„ä¸­

5. **Save** the file

#### Step 5: Add Your Books

1. Copy your Japanese book files into the `input/` folder
2. Supported formats:
   - `.epub` - E-book format (recommended)
   - `.txt` - Plain text (must be UTF-8 encoded)
   - `.md` - Markdown files

#### Step 6: Run the Program

1. **Double-click** `app.exe` to start
2. A console window will open showing progress
3. Wait for processing to complete (time depends on book size)
4. Check the `output/` folder for your results

#### What If It Doesn't Work?

If you encounter errors:
- See the [Troubleshooting](#-troubleshooting) section
- Check the `log/` folder for detailed error messages
- If GPU-related issues occur, consider [Method 2](#method-2-clone-repository-for-developers--gpu-users)

---

### Method 2: Clone Repository (For Developers & GPU Users)

Remember to clone https://huggingface.co/neavo/keyword_gacha_multilingual_ner to "\BookTerm Gacha\resource\kg_ner_bf16" (Since "model.safetensors" is too large for GitHub)

Choose this method if:
- âœ… You have an NVIDIA GPU and want **faster processing** (3-10x speedup)
- âœ… The release version **doesn't recognize your GPU**
- âœ… You want to **modify the code** or contribute to development
- âœ… You want to use a **different Python version** or environment
- âœ… You're experiencing **compatibility issues** with the release version

#### Prerequisites

Before starting, make sure you have:

| Requirement | How to Check | How to Install |
|-------------|--------------|----------------|
| **Python 3.10+** | `python --version` | [python.org](https://www.python.org/downloads/) |
| **Git** (optional) | `git --version` | [git-scm.com](https://git-scm.com/) |
| **NVIDIA GPU** (optional) | `nvidia-smi` | Driver from [nvidia.com](https://www.nvidia.com/drivers/) |
| **CUDA Toolkit** (for GPU) | `nvcc --version` | [CUDA Downloads](https://developer.nvidia.com/cuda-downloads) |

#### Step 1: Clone or Download the Repository

**Option A: Using Git (Recommended)**
```bash
# Open Command Prompt or PowerShell
git clone https://github.com/1235357/BookTermGacha.git
cd BookTermGacha
```

**Option B: Download ZIP**
1. Go to the repository page
2. Click "Code" â†’ "Download ZIP"
3. Extract to your preferred location
4. Open Command Prompt/PowerShell and navigate to the folder:
```bash
cd C:\path\to\BookTermGacha
```

#### Step 2: Create a Virtual Environment (Highly Recommended)

A virtual environment keeps this project's dependencies separate from other Python projects.

**Windows (Command Prompt):**
```bash
# Create virtual environment
python -m venv venv

# Activate it
venv\Scripts\activate

# You should see (venv) at the start of your command line
```

**Windows (PowerShell):**
```powershell
# Create virtual environment
python -m venv venv

# Activate it (you may need to allow script execution first)
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
.\venv\Scripts\Activate.ps1
```

**Linux/Mac:**
```bash
# Create virtual environment
python3 -m venv venv

# Activate it
source venv/bin/activate
```

> âš ï¸ **Important**: Always activate the virtual environment before running commands!

#### Step 3: Install PyTorch (CRITICAL for GPU Users)

This is the **most important step** for GPU acceleration. You must install PyTorch with the correct CUDA version **BEFORE** installing other dependencies.

**First, check your CUDA version:**
```bash
nvidia-smi
```

Look for "CUDA Version" in the output (e.g., "CUDA Version: 12.4").

**Then install PyTorch with matching CUDA:**

| Your CUDA Version | Installation Command |
|-------------------|---------------------|
| **CUDA 12.6** (RTX 40 series) | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126` |
| **CUDA 12.4** | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124` |
| **CUDA 12.1** | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121` |
| **CUDA 11.8** (GTX 10/16/20 series) | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118` |
| **No GPU / CPU Only** | `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu` |

**Example for CUDA 12.4:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

#### Step 4: Install Other Dependencies

```bash
pip install -r requirements.txt
```

This will install all required packages:
- `transformers` - For BERT NER model
- `openai` - For LLM API calls
- `ebooklib` - For EPUB reading
- `rich` - For beautiful console output
- `pykakasi` - For Japanese text processing
- And more...

#### Step 5: Verify Your Installation

Run these commands to make sure everything is working:

```bash
# Check Python version
python --version

# Check if PyTorch sees your GPU
python -c "import torch; print(f'PyTorch Version: {torch.__version__}')"
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\"}')"

# Check transformers
python -c "from transformers import AutoModel; print('Transformers: OK')"

# Check other dependencies
python -c "import openai, ebooklib, rich; print('All dependencies: OK')"
```

**Expected output (with GPU):**
```
PyTorch Version: 2.5.1+cu124
CUDA Available: True
GPU Device: NVIDIA GeForce RTX 4090
Transformers: OK
All dependencies: OK
```

**Expected output (CPU only):**
```
PyTorch Version: 2.5.1+cpu
CUDA Available: False
GPU Device: CPU only
Transformers: OK
All dependencies: OK
```

#### Step 6: Configure Your API Key

Same as Method 1 - edit `config.json` with your Zhipu AI API key.

#### Step 7: Run the Program

```bash
# Make sure virtual environment is activated
# (venv) should appear in your prompt

python app.py
```

#### Common Issues and Solutions

| Issue | Solution |
|-------|----------|
| `CUDA Available: False` even with GPU | Reinstall PyTorch with correct CUDA version |
| `ModuleNotFoundError` | Make sure venv is activated, run `pip install -r requirements.txt` |
| Permission errors on Windows | Run Command Prompt as Administrator |
| Python not found | Add Python to PATH or use full path |

---

## âš™ï¸ Configuration Guide

The `config.json` file controls all settings. Here's a detailed explanation:

### Platform Configuration (æ ¸å¿ƒé…ç½®)

```json
{
    "activate_platform": 1,
    "platforms": [...]
}
```

| Setting | Description | Values |
|---------|-------------|--------|
| `activate_platform` | å½“å‰æ¿€æ´»çš„å¹³å° ID | `0`, `1`, `2`, `3` |
| `platforms` | å¹³å°é…ç½®æ•°ç»„ | è§ä¸‹æ–¹è¯¦ç»†è¯´æ˜ |

### Platform Object Structure (å¹³å°é…ç½®ç»“æ„)

```json
{
    "id": 1,
    "name": "NVIDIA-DeepSeek-V3.2",
    "api_url": "https://integrate.api.nvidia.com/v1",
    "api_key": ["key1", "key2", "key3"],
    "model": "deepseek-ai/deepseek-v3.2",
    "thinking": true,
    "top_p": 0.95,
    "temperature": 0.95,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0,
    "description": "æè¿°æ–‡å­—"
}
```

| Field | Description | Example |
|-------|-------------|---------|
| `id` | å¹³å°å”¯ä¸€æ ‡è¯† | `0`, `1`, `2`, `3` |
| `name` | å¹³å°æ˜¾ç¤ºåç§° | `"NVIDIA-DeepSeek-V3.2"` |
| `api_url` | API ç«¯ç‚¹ URL | `"https://integrate.api.nvidia.com/v1"` |
| `api_key` | API Key æ•°ç»„ (æ”¯æŒå¤š Key è½®è¯¢) | `["key1", "key2"]` |
| `model` | æ¨¡å‹åç§° | `"deepseek-ai/deepseek-v3.2"` |
| `thinking` | æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ | `true` / `false` |

### Pre-configured Platforms (é¢„é…ç½®å¹³å°)

| ID | Platform | API URL | Model | Free Tier |
|----|----------|---------|-------|-----------|
| `0` | **æ™ºè°± GLM** | `https://open.bigmodel.cn/api/paas/v4` | `glm-4.6v-flash` | âœ… å®Œå…¨å…è´¹ |
| `1` | **NVIDIA Build** | `https://integrate.api.nvidia.com/v1` | `deepseek-ai/deepseek-v3.2` | âœ… 1000æ¬¡/å¤© |
| `2` | **ModelScope V3.2** | `https://api-inference.modelscope.cn/v1/` | `deepseek-ai/DeepSeek-V3.2` | âœ… å……è¶³é¢åº¦ |
| `3` | **ModelScope R1** | `https://api-inference.modelscope.cn/v1/` | `deepseek-ai/DeepSeek-R1-0528` | âœ… å……è¶³é¢åº¦ |

### Multi-API Key Rotation (å¤š Key è½®è¯¢)

```json
"api_key": [
    "nvapi-key1-xxxxx",
    "nvapi-key2-xxxxx",
    "nvapi-key3-xxxxx",
    "nvapi-key4-xxxxx",
    "nvapi-key5-xxxxx"
]
```

**ç‰¹æ€§ï¼š**
- ğŸ”„ **è‡ªåŠ¨è½®è¯¢**: è¯·æ±‚è‡ªåŠ¨åˆ†é…åˆ°ä¸åŒçš„ API Key
- ğŸš« **æ™ºèƒ½é»‘åå•**: è¢«å°ç¦çš„ Key è‡ªåŠ¨åŠ å…¥é»‘åå•ï¼Œä¸å½±å“å…¶ä»– Key
- âš¡ **å¹¶å‘æå‡**: 5 ä¸ª Key = 5 å€ååé‡
- ğŸ“Š **çŠ¶æ€æ˜¾ç¤º**: å¯åŠ¨æ—¶æ˜¾ç¤ºå¯ç”¨ Key æ•°é‡

### Optional Settings (å¯é€‰é…ç½®)

```json
{
    "count_threshold": [2, "å‡ºç°æ¬¡æ•°é˜ˆå€¼"],
    "score_threshold": [0.60, "NER ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0)"],
    "max_context_samples": [5, "ä¸Šä¸‹æ–‡é‡‡æ ·æ®µè½æ•°"],
    "tokens_per_sample": [512, "æ¯æ®µæœ€å¤§ token æ•°"],
    "ner_target_types": [["PER", "LOC"], "æå–çš„å®ä½“ç±»å‹"],
    "request_timeout": [1800, "API è¶…æ—¶æ—¶é—´(ç§’)"],
    "request_frequency_threshold": [10, "æ¯ç§’æœ€å¤§è¯·æ±‚æ•°"],
    "max_concurrent_requests": [90, "æœ€å¤§å¹¶å‘è¯·æ±‚æ•°"],
    "traditional_chinese_enable": [false, "ç¹ä½“ä¸­æ–‡è¾“å‡º"]
}
```

| Setting | Description | Default | æ¨èå€¼ |
|---------|-------------|---------|--------|
| `count_threshold` | è¯è¯­æœ€å°‘å‡ºç°æ¬¡æ•° | `2` | ä¿æŒé»˜è®¤ |
| `score_threshold` | NER ç½®ä¿¡åº¦é˜ˆå€¼ | `0.60` | `0.50-0.70` |
| `request_frequency_threshold` | æ¯ç§’è¯·æ±‚æ•°ä¸Šé™ | `10` | å¤š Key æ—¶è®¾ä¸º `5-10` |
| `max_concurrent_requests` | æœ€å¤§å¹¶å‘æ•° | `90` | å¤š Key æ—¶å¯å¢åŠ  |
| `traditional_chinese_enable` | ç¹ä½“ä¸­æ–‡ | `false` | å°æ¹¾/é¦™æ¸¯ç”¨æˆ·è®¾ä¸º `true` |

### Adding Custom LLM Providers (æ·»åŠ è‡ªå®šä¹‰å¹³å°)

å¯ä»¥åœ¨ `platforms` æ•°ç»„ä¸­æ·»åŠ ä»»ä½• OpenAI å…¼å®¹çš„ APIï¼š

```json
{
    "id": 4,
    "name": "My-Custom-Provider",
    "api_url": "https://api.example.com/v1",
    "api_key": ["your-api-key"],
    "model": "model-name",
    "thinking": false,
    "top_p": 0.95,
    "temperature": 0.7,
    "description": "è‡ªå®šä¹‰å¹³å°"
}
```

**æ”¯æŒçš„å¹³å°ç±»å‹ï¼š**

| Provider | Base URL | Model | Notes |
|----------|----------|-------|-------|
| **NVIDIA Build** | `https://integrate.api.nvidia.com/v1` | `deepseek-ai/deepseek-v3.2` | ğŸ¥‡ æ¨èï¼Œæ”¯æŒæ€è€ƒæ¨¡å¼ |
| **ModelScope** | `https://api-inference.modelscope.cn/v1/` | `deepseek-ai/DeepSeek-V3.2` | ğŸ¥ˆ é˜¿é‡Œäº‘ç™¾ç‚¼ |
| **Zhipu AI** | `https://open.bigmodel.cn/api/paas/v4` | `glm-4.6v-flash` | ğŸ†“ å®Œå…¨å…è´¹ |
| **DeepSeek** | `https://api.deepseek.com/v1` | `deepseek-chat` | å®˜æ–¹ API |
| **OpenAI** | `https://api.openai.com/v1` | `gpt-4o-mini` | é«˜è´¨é‡ï¼Œé«˜æˆæœ¬ |
| **Local LLM** | `http://localhost:11434/v1` | varies | Ollama ç­‰æœ¬åœ°æ¨¡å‹ |

---

## ğŸ“– How to Use

### Step-by-Step Workflow

#### 1. Prepare Your Input Files

- Place your Japanese book files in the `input/` folder
- Supported formats: `.epub`, `.txt`, `.md`
- Multiple files can be processed in one run
- File names can be in any language (Japanese, English, Chinese, etc.)

**Example:**
```
input/
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1.epub
â”œâ”€â”€ Unnamed Memory.epub
â”œâ”€â”€ my_novel.txt
â””â”€â”€ another_book.md
```

#### 2. Run the Program

**Release version:** Double-click `app.exe`

**Development version:** 
```bash
python app.py
```

#### 3. Monitor Progress

The program shows real-time progress:

```
â•­â”€ BookTerm Gacha v0.1.0 â”€â•®
â”‚ Processing: è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1.epub
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[1/4] Loading book...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% Reading EPUB

[2/4] NER Entity Extraction (BERT)...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% Found 127 entities

[3/4] Context Translation (LLM)...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  42% Processing entity 54/127

[4/4] Semantic Analysis (LLM)...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% Analysis complete

âœ“ Processing complete! Check output/ folder.
```

#### 4. Collect Your Results

After processing, check the `output/` folder:

```
output/
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_è§’è‰²_è¯å…¸.json      # Character dictionary
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_è§’è‰²_æœ¯è¯­è¡¨.json    # LinguaGacha glossary
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_è§’è‰²_galtransl.txt  # GalTransl format
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_è§’è‰²_æ—¥å¿—.txt       # Detailed log
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_åœ°ç‚¹_è¯å…¸.json      # Location dictionary
â”œâ”€â”€ è»¢ç”Ÿã—ãŸã‚‰ã‚¹ãƒ©ã‚¤ãƒ ã ã£ãŸä»¶ 1_åœ°ç‚¹_æœ¯è¯­è¡¨.json    # Location glossary
â””â”€â”€ çµæœæ¤œæŸ»_æŠ¥å‘Š.json                              # Quality report
```

---

## ğŸ“Š Understanding the Output

### Dictionary Format (`_è¯å…¸.json`)

The main output - a list of terms with translations:

```json
[
    {
        "src": "ãƒªãƒ ãƒ«",
        "dst": "åˆ©å§†é²",
        "info": "ä¸»è§’ï¼Œè½¬ç”Ÿæˆå²è±å§†çš„æ—¥æœ¬äººï¼Œåæˆä¸ºé­”ç‹ã€‚"
    },
    {
        "src": "ã‚·ã‚º",
        "dst": "é™",
        "info": "å¥³æ€§å†’é™©è€…ï¼Œè¢«å¬å”¤åˆ°å¼‚ä¸–ç•Œçš„æ—¥æœ¬äººã€‚"
    }
]
```

| Field | Description |
|-------|-------------|
| `src` | Original Japanese name (source) |
| `dst` | Chinese translation (destination) |
| `info` | Character description/summary |

### LinguaGacha Format (`_æœ¯è¯­è¡¨.json`)

Ready to import into [LinguaGacha](https://github.com/neavo/LinguaGacha):

```json
[
    {
        "src": "ãƒªãƒ ãƒ«",
        "dst": "åˆ©å§†é²",
        "info": "è§’è‰² - ç”· - ä¸»è§’ï¼Œè½¬ç”Ÿæˆå²è±å§†çš„æ—¥æœ¬äºº..."
    }
]
```

### GalTransl Format (`_galtransl.txt`)

For use with [GalTransl](https://github.com/xd2333/GalTransl):

```
ãƒªãƒ ãƒ« | åˆ©å§†é²
ã‚·ã‚º | é™
ãƒ´ã‚§ãƒ«ãƒ‰ãƒ© | ç»´é²å¤šæ‹‰
```

### Quality Report (`çµæœæ¤œæŸ»_æŠ¥å‘Š.json`)

Automatically checks for issues:

```json
{
    "å‡åæ®‹ç•™": ["ã‚¨ãƒ«ãƒ•ã®é‡Œ â†’ ç²¾çµã®é‡Œ"],
    "æœªç¿»è¯‘æ¡ç›®": ["ã‚¢ãƒ«ãƒ“ã‚¹"],
    "ç›¸ä¼¼åº¦é—®é¢˜": []
}
```

---

## ğŸ”§ Troubleshooting

### Common Problems and Solutions

#### "API key invalid" Error

**Problem:** The program says your API key is invalid.

**Solutions:**
1. Double-check your API key in `config.json`
2. Make sure there are no extra spaces
3. Verify the key is active on [bigmodel.cn](https://bigmodel.cn/)
4. Check if you've exceeded the free tier limits

#### GPU Not Detected (Release Version)

**Problem:** The release version runs on CPU even though you have an NVIDIA GPU.

**Why:** The release is bundled with CPU-only PyTorch for maximum compatibility.

**Solution:** Use [Method 2](#method-2-clone-repository-for-developers--gpu-users) to install with proper CUDA support.

#### "CUDA out of memory" Error

**Problem:** GPU runs out of memory during NER processing.

**Solutions:**
1. Close other GPU-intensive applications
2. Process smaller files or split large books
3. The program will automatically fall back to CPU if needed

#### "Module not found" Error

**Problem:** Python can't find required packages.

**Solutions:**
```bash
# Make sure venv is activated
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Reinstall dependencies
pip install -r requirements.txt
```

#### Slow Processing

**Tips to speed up:**
1. Use GPU (Method 2 installation)
2. Reduce `max_context_samples` in config
3. Increase `count_threshold` to process fewer entities
4. Process books one at a time

#### Japanese Text Displays as Garbled Characters

**Problem:** Output shows `???` or garbled text.

**Solutions:**
1. Make sure your terminal supports UTF-8
2. For Windows: Run `chcp 65001` before starting
3. Open output files with UTF-8 encoding (use VS Code or Notepad++)

---

## ğŸ”¬ Technical Details

### How It Works

BookTerm Gacha uses a **4-stage pipeline**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BookTerm Gacha Pipeline                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Stage 1: NER Extraction (BERT)                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  â€¢ Reads book text from EPUB/TXT/MD                         â”‚
â”‚  â€¢ BERT model identifies named entities                     â”‚
â”‚  â€¢ Filters: Only keeps names WITH kana characters           â”‚
â”‚  â€¢ Output: List of potential character/location names       â”‚
â”‚                                                             â”‚
â”‚                         â†“                                   â”‚
â”‚                                                             â”‚
â”‚  Stage 2: Context Sampling                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  â€¢ For each entity, finds paragraphs where it appears       â”‚
â”‚  â€¢ Samples N context paragraphs (configurable)              â”‚
â”‚  â€¢ Prepares context for LLM analysis                        â”‚
â”‚                                                             â”‚
â”‚                         â†“                                   â”‚
â”‚                                                             â”‚
â”‚  Stage 3: LLM Analysis (Zhipu GLM)                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  â€¢ Sends entity + context to LLM                            â”‚
â”‚  â€¢ LLM returns: translation, gender, category, summary      â”‚
â”‚  â€¢ Validation: Checks for kana residue, degradation         â”‚
â”‚  â€¢ Retry logic: Up to 8 retries with fallback               â”‚
â”‚                                                             â”‚
â”‚                         â†“                                   â”‚
â”‚                                                             â”‚
â”‚  Stage 4: Output Generation                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  â€¢ ResultChecker validates all entries                      â”‚
â”‚  â€¢ Generates multiple output formats                        â”‚
â”‚  â€¢ Creates quality report                                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why BERT + LLM?

| Component | Task | Why This Choice |
|-----------|------|-----------------|
| **BERT NER** | Find names in text | Fast, accurate, runs locally |
| **Zhipu GLM** | Translate & analyze | Better context understanding, creative translation |

Using both gives the **best of both worlds** - fast extraction with intelligent analysis.

### Smart Kana Handling

The system handles Japanese kana intelligently:

- **Strict Mode**: Flags any kana remaining in translations
- **Tolerance**: Allows certain kana (ã£, ãƒ¼, ãƒ¶) that appear in place names
- **Fallback**: After 5 failed retries, force-transliterates using romaji â†’ Chinese mapping

---

## ğŸ“‹ Release Notes

### Version 0.2.0 - Multi-Platform & API Key Rotation

**Release Date:** January 2026

**ğŸ‰ Major Version Upgrade!**

This release brings massive performance improvements with multi-platform LLM support and API key rotation.

#### âœ¨ New Features

- **ğŸŒ Multi-Platform Support**: NVIDIA Build, ModelScope (é˜¿é‡Œäº‘ç™¾ç‚¼), Zhipu GLM å…¨é¢æ”¯æŒ
- **ğŸ”„ API Key Rotation**: å¤š API Key è‡ªåŠ¨è½®è¯¢ï¼Œå¤§å¹…æå‡å¹¶å‘ååé‡
- **ğŸš« Smart Blacklist**: è‡ªåŠ¨æ£€æµ‹è¢«å°ç¦çš„ Key å¹¶åŠ å…¥é»‘åå•
- **ğŸ’¡ Deep Thinking Mode**: æ”¯æŒ DeepSeek V3.2/R1 çš„æ¨ç†æ¨¡å¼
- **âš¡ NVIDIA DeepSeek**: é¦–é€‰æ¨èå¹³å°ï¼Œé€Ÿåº¦ä¸è´¨é‡å…¼å…·
- **ğŸ“Š Enhanced Status**: å¯åŠ¨æ—¶æ˜¾ç¤ºå¹³å°ä¿¡æ¯å’Œå¯ç”¨ Key æ•°é‡

#### ğŸ”§ Technical Improvements

- å…¨æ–°çš„å¤šå¹³å°é…ç½®æ ¼å¼ (`platforms` æ•°ç»„)
- æ™ºèƒ½å¹³å°æ£€æµ‹ (NVIDIA/ModelScope/Zhipu è‡ªåŠ¨è¯†åˆ«)
- æµå¼å“åº”ä¼˜åŒ–ï¼Œæ”¯æŒ `reasoning_content` æå–
- API Key é»‘åå•æœºåˆ¶ (è‡ªåŠ¨å¤„ç† 403 é”™è¯¯)
- å¹¶å‘æ§åˆ¶ä¼˜åŒ– (`max_concurrent_requests` é…ç½®)

#### ğŸ“¦ Pre-configured Platforms

| Platform | Model | Features |
|----------|-------|----------|
| **æ™ºè°± GLM-4.6v-flash** | å…è´¹æ— é™åˆ¶ | æ·±åº¦æ€è€ƒ |
| **NVIDIA DeepSeek V3.2** | 5 Key è½®è¯¢ | é«˜é€Ÿæ¨ç† |
| **ModelScope DeepSeek V3.2** | 5 Key è½®è¯¢ | é˜¿é‡Œäº‘ç™¾ç‚¼ |
| **ModelScope DeepSeek R1** | æ¨ç†æ¨¡å‹ | æ·±åº¦æ€è€ƒ |

#### âš ï¸ Breaking Changes

- `config.json` æ ¼å¼å·²æ›´æ–°ä¸ºå¤šå¹³å°æ ¼å¼
- æ—§é…ç½®éœ€è¦è¿ç§»åˆ°æ–°çš„ `platforms` æ•°ç»„æ ¼å¼
- æ–°å¢ `activate_platform` å­—æ®µæŒ‡å®šæ´»åŠ¨å¹³å°

---

### Version 0.1.0-Zhipu_GLM-Optimize

**Release Date:** January 2026

**ğŸ‰ First Major Release!**

This is the first stable release of BookTerm Gacha, specifically optimized for the Zhipu GLM API.

#### âœ¨ New Features

- **Zhipu GLM Optimization**: Fine-tuned prompts and settings for best results with GLM-4-Flash
- **Complete Workflow**: Full pipeline from book input to glossary output
- **Multiple Output Formats**: JSON dictionary, LinguaGacha glossary, GalTransl format
- **Rich Progress Display**: Beautiful console output with progress bars
- **Quality Validation**: Automatic checking for kana residue and translation issues
- **Smart Retry Logic**: Intelligent retry with forced transliteration fallback

#### ğŸ”§ Technical Improvements

- Optimized NER filtering (only processes kana-containing entities)
- Place name particle handling (ãƒ¶, ã®, etc.)
- Context-aware kana detection with tolerance rules
- Comprehensive error handling and logging

#### ğŸ“¦ What's Included

- Pre-configured for Zhipu GLM API (free tier available)
- BERT NER model for Japanese entity extraction
- Blacklist filters for common words (particles, pronouns, etc.)
- LLM prompts optimized for terminology extraction

#### âš ï¸ Known Limitations

- Release version uses CPU-only PyTorch (use dev setup for GPU)
- Optimized for Japanese â†’ Chinese (other languages may work but untested)
- Large books (500k+ characters) may take 30+ minutes

#### ğŸ™ Based On

- [KeywordGacha v0.13.1](https://github.com/neavo/KeywordGacha) - Core workflow and NER model
- [LinguaGacha](https://github.com/neavo/LinguaGacha) - Validation patterns and text utilities

---

## ğŸ™ Acknowledgments

This project wouldn't be possible without:

- **[KeywordGacha](https://github.com/neavo/KeywordGacha)** by neavo - The original project that inspired this fork
- **[LinguaGacha](https://github.com/neavo/LinguaGacha)** by neavo - Design patterns and validation logic
- **[NVIDIA Build](https://build.nvidia.com/)** - High-performance DeepSeek API
- **[ModelScope é˜¿é‡Œäº‘ç™¾ç‚¼](https://www.modelscope.cn/)** - Generous free LLM API
- **[Zhipu AI / BigModel](https://bigmodel.cn/)** - Free LLM API that makes this accessible to everyone
- **[Hugging Face Transformers](https://huggingface.co/)** - The BERT NER pipeline
- **The Japanese Literatures Translation Community** - For feedback and testing

---

## ğŸ“„ License

This project is released under the **MIT License**.

You are free to:
- âœ… Use commercially
- âœ… Modify
- âœ… Distribute
- âœ… Use privately

With the condition that you include the original license and copyright notice.

**If you use this tool in your translation work, please credit appropriately.**

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. **Report Bugs**: Open an issue describing the problem
2. **Suggest Features**: Share your ideas for improvements
3. **Submit Code**: Fork, make changes, submit a pull request
4. **Improve Docs**: Help make this README even better
5. **Share**: Tell others about this tool!

### Development Setup

```bash
# Clone the repo
git clone https://github.com/1235357/BookTermGacha.git
cd BookTermGacha

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows

# Install with GPU support (see Method 2)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt

# Run tests
python -m pytest -v

# Make your changes and submit a PR!
```

---

## â“ FAQ

**Q: å“ªä¸ª LLM å¹³å°æœ€æ¨èï¼Ÿ**
A: NVIDIA Build çš„ DeepSeek V3.2 æ˜¯é¦–é€‰ï¼Œé€Ÿåº¦å¿«è´¨é‡é«˜ã€‚ModelScopeï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼‰æ˜¯å¤‡é€‰ï¼Œæ™ºè°± GLM å®Œå…¨å…è´¹é€‚åˆå…¥é—¨ã€‚

**Q: å¤š API Key è½®è¯¢æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ**
A: 5 ä¸ª Key = 5 å€ååé‡ï¼å¯ä»¥å¤§å¹…ç¼©çŸ­å¤„ç†æ—¶é—´ã€‚å»ºè®®æ¯ä¸ªå¹³å°å‡†å¤‡ 3-5 ä¸ª Keyã€‚

**Q: API Key è¢«å°ç¦äº†æ€ä¹ˆåŠï¼Ÿ**
A: ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶å°†è¢«å°çš„ Key åŠ å…¥é»‘åå•ï¼Œä¸å½±å“å…¶ä»– Key ç»§ç»­å·¥ä½œã€‚

**Q: Is the Zhipu API really free?**
A: Yes! The GLM-4.6v-Flash model has a free tier with no limits. Perfect for getting started.

**Q: Can I use this for Korean/English books?**
A: Currently optimized for Japanese â†’ Chinese. Other languages may work but are untested.

**Q: How long does processing take?**
A: Depends on book size and API Key count. With 5 Keys, a typical 100k character book takes 3-8 minutes.

**Q: å¦‚ä½•æ·»åŠ æ–°çš„ LLM å¹³å°ï¼Ÿ**
A: åœ¨ `config.json` çš„ `platforms` æ•°ç»„ä¸­æ·»åŠ æ–°å¹³å°é…ç½®ï¼Œè®¾ç½® `activate_platform` ä¸ºæ–°å¹³å°çš„ IDã€‚

**Q: Why are some names not extracted?**
A: The NER model focuses on names with kana. Pure kanji names (like ç”°ä¸­) are skipped as they don't need transliteration.

**Q: æ·±åº¦æ€è€ƒæ¨¡å¼ï¼ˆthinkingï¼‰æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ**
A: å¯ç”¨åæ¨¡å‹ä¼šè¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†åˆ†æï¼Œç¿»è¯‘è´¨é‡æ›´é«˜ï¼Œä½†é€Ÿåº¦ç•¥æ…¢ã€‚æ¨èä¿æŒå¼€å¯ã€‚

---

<p align="center">
  <strong>ğŸ“š BookTerm Gacha v0.2.0</strong>
  <br>
  <em>Multi-Platform LLM Support with API Key Rotation</em>
  <br>
  <em>Transforming Japanese Literatures into Translation-Ready Glossaries</em>
  <br><br>
  Made for the Japanese Literatures Translation Community
</p>
