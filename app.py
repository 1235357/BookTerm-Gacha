"""
BookTerm Gacha - Main Application Entry Point
==============================================

A specialized term extraction tool for books (EPUB/TXT/MD) that uses
BERT-based NER and LLM semantic analysis to generate translation glossaries.

Main Workflow:
    1. Read input files (EPUB, TXT, MD, etc.) from the input folder
    2. Extract named entities using BERT NER model
    3. Analyze terms with LLM to determine categories and translations
    4. Generate output files (JSON, MD, XLSX) for use in translation tools

Key Features:
    - Multi-language support: Chinese, Japanese, Korean, English
    - GPU acceleration for NER (CUDA)
    - Configurable LLM backends (OpenAI-compatible APIs)
    - Quality checks for kana residue and similarity issues
    - Traditional/Simplified Chinese output options

Configuration:
    All settings are loaded from config.json in the application directory.
    See README.md for detailed configuration options.

Usage:
    python app.py

Based on KeywordGacha v0.13.1 by neavo
https://github.com/neavo/KeywordGacha
"""

import os
import sys
import copy
import json
import asyncio
import subprocess
from types import SimpleNamespace

# ============== Windows æ§åˆ¶å° UTF-8 ç¼–ç è®¾ç½®ï¼ˆå¿…é¡»åœ¨æœ€å¼€å§‹ï¼‰ ==============
if sys.platform == 'win32':
    try:
        import ctypes
        # è®¾ç½®æ§åˆ¶å°è¾“å‡ºä»£ç é¡µä¸º UTF-8
        ctypes.windll.kernel32.SetConsoleOutputCP(65001)
        ctypes.windll.kernel32.SetConsoleCP(65001)
    except:
        pass
    
    # è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç 
    if hasattr(sys.stdout, 'reconfigure'):
        try:
            sys.stdout.reconfigure(encoding='utf-8', errors='replace')
            sys.stderr.reconfigure(encoding='utf-8', errors='replace')
        except:
            pass


def run_environment_check() -> bool:
    """
    è¿è¡Œç¯å¢ƒæ£€æŸ¥å’Œè‡ªåŠ¨ä¿®å¤
    
    Returns:
        True å¦‚æœç¯å¢ƒå°±ç»ªï¼ŒFalse å¦‚æœå¤±è´¥
    """
    try:
        from module.EnvChecker import check_environment
        return check_environment(auto_repair=True)
    except ImportError:
        # Rich æœªå®‰è£…ï¼Œå…ˆå®‰è£…åŸºç¡€åŒ…
        print("=" * 60)
        print("  é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...")
        print("=" * 60)
        
        mirrors = [
            "https://pypi.tuna.tsinghua.edu.cn/simple",
            "https://mirrors.aliyun.com/pypi/simple",
        ]
        
        for mirror in mirrors:
            try:
                result = subprocess.run(
                    [sys.executable, "-m", "pip", "install", "-i", mirror, 
                     "--trusted-host", mirror.split("//")[1].split("/")[0],
                     "rich", "loguru"],
                    capture_output=True,
                    text=True,
                    timeout=120
                )
                if result.returncode == 0:
                    print(f"  âœ“ åŸºç¡€åŒ…å®‰è£…æˆåŠŸ")
                    break
            except Exception:
                continue
        
        # é‡æ–°å°è¯•
        try:
            from module.EnvChecker import check_environment
            return check_environment(auto_repair=True)
        except Exception as e:
            print(f"\nâŒ ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            print("   è¯·æ‰‹åŠ¨è¿è¡Œ: pip install -r requirements.txt")
            return False


# ============== ç¯å¢ƒæ£€æŸ¥ï¼ˆå¿…é¡»åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰ï¼‰ ==============
if __name__ == "__main__":
    if not run_environment_check():
        print("\nâŒ ç¯å¢ƒæ£€æµ‹å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…ä¾èµ–åé‡è¯•")
        print("   å‚è€ƒå‘½ä»¤: pip install -r requirements.txt")
        os.system("pause")
        sys.exit(1)


# ============== ç¯å¢ƒæ£€æŸ¥é€šè¿‡åï¼Œå¯¼å…¥æ‰€æœ‰ä¾èµ– ==============
from rich import box
from rich.table import Table
from rich.prompt import Prompt
from rich.traceback import install

from model.LLM import LLM
from model.NER import NER
from model.Word import Word
from module.LogHelper import LogHelper
from module.ProgressHelper import ProgressHelper
from module.TestHelper import TestHelper
from module.FileManager import FileManager
from module.Text.TextHelper import TextHelper


# ============== é…ç½®å¸¸é‡ï¼ˆé»˜è®¤å€¼ï¼Œä¼šè¢« config.json è¦†ç›–ï¼‰ ==============
SCORE_THRESHOLD = 0.60          # ç½®ä¿¡åº¦é˜ˆå€¼
MAX_DISPLAY_LENGTH = 32         # æœ¯è¯­æœ€å¤§æ˜¾ç¤ºé•¿åº¦

# åˆå¹¶è¯è¯­
def merge_words(words: list[Word]) -> list[Word]:
    words_unique = {}
    for word in words:
        words_unique.setdefault(word.surface, []).append(word)

    words_merged = []
    for v in words_unique.values():
        word = v[0]
        word.score = min(0.9999, max(w.score for w in v))
        words_merged.append(word)

    return sorted(words_merged, key = lambda x: x.count, reverse = True)


# è¿‡æ»¤è¶…é•¿æœ¯è¯­ï¼ˆå€Ÿé‰´è‡ª V0.20.2ï¼‰
def filter_by_display_length(words: list[Word], max_length: int = MAX_DISPLAY_LENGTH) -> list[Word]:
    """è¿‡æ»¤æ˜¾ç¤ºé•¿åº¦è¶…è¿‡é˜ˆå€¼çš„æœ¯è¯­"""
    filtered = []
    for word in words:
        display_length = TextHelper.get_display_lenght(word.surface)
        if display_length <= max_length:
            filtered.append(word)
        else:
            LogHelper.debug(f"[é•¿åº¦è¿‡æ»¤] è¿‡æ»¤è¶…é•¿æœ¯è¯­: {word.surface} (é•¿åº¦: {display_length})")
    return filtered

# æœç´¢å‚è€ƒæ–‡æœ¬ï¼Œå¹¶æŒ‰å‡ºç°æ¬¡æ•°æ’åº
def search_for_context(words: list[Word], input_lines: list[str]) -> list[Word]:
    # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…åç»­çš„ä¿®æ”¹å½±å“åŸå§‹æ•°æ®
    input_lines_ex = copy.copy(input_lines)

    # æŒ‰å®ä½“è¯è¯­çš„é•¿åº¦é™åºæ’åº
    words = sorted(words, key = lambda v: len(v.surface), reverse = True)

    LogHelper.print("")
    with ProgressHelper.get_progress() as progress:
        pid = progress.add_task("æœç´¢å‚è€ƒæ–‡æœ¬", total = len(words))

        # æœç´¢å‚è€ƒæ–‡æœ¬
        for word in words:
            # æ‰¾å‡ºåŒ¹é…çš„è¡Œ
            index = {i for i, line in enumerate(input_lines_ex) if word.surface in line}

            # è·å–åŒ¹é…çš„å‚è€ƒæ–‡æœ¬ï¼Œå»é‡ï¼Œå¹¶æŒ‰é•¿åº¦é™åºæ’åº
            word.context = {line for i, line in enumerate(input_lines) if i in index}
            word.context = sorted(list(word.context), key = lambda v: len(v), reverse = True)
            word.count = len(word.context)
            word.group = "æœªçŸ¥ç±»å‹"

            # æ©ç›–å·²å‘½ä¸­çš„å®ä½“è¯è¯­æ–‡æœ¬ï¼Œé¿å…å…¶å­ä¸²é”™è¯¯çš„ä¸çˆ¶ä¸²åŒ¹é…
            input_lines_ex = [
                line.replace(word.surface, len(word.surface) * "#")  if i in index else line
                for i, line in enumerate(input_lines_ex)
            ]

            # æ›´æ–°è¿›åº¦æ¡
            progress.update(pid, advance = 1)
    LogHelper.print("")

    # æŒ‰å‡ºç°æ¬¡æ•°é™åºæ’åº
    return sorted(words, key = lambda x: x.count, reverse = True)

# æŒ‰ç½®ä¿¡åº¦è¿‡æ»¤è¯è¯­
def filter_words_by_score(words: list[Word], threshold: float) -> list[Word]:
    return [word for word in words if word.score >= threshold]

# æŒ‰å‡ºç°æ¬¡æ•°è¿‡æ»¤è¯è¯­
def filter_words_by_count(words: list[Word], threshold: float) -> list[Word]:
    return [word for word in words if word.count >= max(1, threshold)]

# è·å–æŒ‡å®šç±»å‹çš„è¯
def get_words_by_type(words: list[Word], group: str) -> list[Word]:
    return [word for word in words if word.group == group]

# ç§»é™¤æŒ‡å®šç±»å‹çš„è¯
def remove_words_by_type(words: list[Word], group: str) -> list[Word]:
    return [word for word in words if word.group != group]

# å¼€å§‹å¤„ç†æ–‡æœ¬
async def process_text(llm: LLM, ner: NER, file_manager: FileManager, config: SimpleNamespace, language: int) -> None:
    # åˆå§‹åŒ–
    words = []

    # è¯»å–è¾“å…¥æ–‡ä»¶
    input_lines, names, nicknames = file_manager.read_lines_from_input_file(language)

    # æŸ¥æ‰¾å®ä½“è¯è¯­
    LogHelper.info("å³å°†å¼€å§‹æ‰§è¡Œ [æŸ¥æ‰¾å®ä½“è¯è¯­] ...")
    words, fake_name_mapping = ner.search_for_entity(input_lines, names, nicknames, language)

    # åˆå¹¶ç›¸åŒè¯æ¡
    words = merge_words(words)

    # è°ƒè¯•åŠŸèƒ½
    TestHelper.check_score_threshold(words, "log_score_threshold.log")

    # ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤
    LogHelper.info(f"å³å°†å¼€å§‹æ‰§è¡Œ [ç½®ä¿¡åº¦é˜ˆå€¼]ï¼Œå½“å‰ç½®ä¿¡åº¦çš„é˜ˆå€¼ä¸º {SCORE_THRESHOLD:.4f} ...")
    words = filter_words_by_score(words, SCORE_THRESHOLD)
    LogHelper.info("[ç½®ä¿¡åº¦é˜ˆå€¼] å·²å®Œæˆ ...")

    # æœç´¢å‚è€ƒæ–‡æœ¬
    LogHelper.info("å³å°†å¼€å§‹æ‰§è¡Œ [æœç´¢å‚è€ƒæ–‡æœ¬] ...")
    words = search_for_context(words, input_lines)

    # å‡ºç°æ¬¡æ•°é˜ˆå€¼è¿‡æ»¤
    LogHelper.info(f"å³å°†å¼€å§‹æ‰§è¡Œ [å‡ºç°æ¬¡æ•°é˜ˆå€¼]ï¼Œå½“å‰å‡ºç°æ¬¡æ•°çš„é˜ˆå€¼ä¸º {config.count_threshold} ...")
    words = filter_words_by_count(words, config.count_threshold)
    LogHelper.info("[å‡ºç°æ¬¡æ•°é˜ˆå€¼] å·²å®Œæˆ ...")

    # é•¿åº¦è¿‡æ»¤ï¼ˆè¿‡æ»¤æ˜¾ç¤ºé•¿åº¦è¶…è¿‡32çš„è¶…é•¿æœ¯è¯­ï¼Œå€Ÿé‰´ V0.20.2ï¼‰
    LogHelper.info(f"å³å°†å¼€å§‹æ‰§è¡Œ [é•¿åº¦è¿‡æ»¤]ï¼Œè¿‡æ»¤æ˜¾ç¤ºé•¿åº¦è¶…è¿‡ {MAX_DISPLAY_LENGTH} çš„æœ¯è¯­ ...")
    original_count = len(words)
    words = filter_by_display_length(words, MAX_DISPLAY_LENGTH)
    filtered_count = original_count - len(words)
    if filtered_count > 0:
        LogHelper.info(f"[é•¿åº¦è¿‡æ»¤] å·²è¿‡æ»¤ {filtered_count} ä¸ªè¶…é•¿æœ¯è¯­ ...")
    LogHelper.info("[é•¿åº¦è¿‡æ»¤] å·²å®Œæˆ ...")

    # è®¾ç½® LLM å¯¹è±¡
    llm.set_language(language)
    llm.set_request_limiter()

    # ============== æ–°å·¥ä½œæµç¨‹ï¼šå…ˆç¿»è¯‘å‚è€ƒæ–‡æœ¬ï¼Œå†è¿›è¡Œè¯ä¹‰åˆ†æ ==============
    # æ ¸å¿ƒæ€è·¯ï¼šè®© LLM åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­å……åˆ†ç†è§£ä¸Šä¸‹æ–‡ï¼Œç„¶ååŸºäºç¿»è¯‘ç»“æœè¿›è¡Œæ ¡å¯¹å®¡æŸ¥
    
    # æ­¥éª¤1ï¼šå‚è€ƒæ–‡æœ¬ç¿»è¯‘ï¼ˆéä¸­æ–‡æ—¶æ‰§è¡Œï¼Œè®© LLM ç†è§£ä¸Šä¸‹æ–‡ï¼‰
    if language != NER.Language.ZH:
        LogHelper.info("å³å°†å¼€å§‹æ‰§è¡Œ [å‚è€ƒæ–‡æœ¬ç¿»è¯‘]ï¼ˆç¬¬ä¸€é˜¶æ®µï¼šç¿»è¯‘ä¸Šä¸‹æ–‡ï¼Œç†è§£è¯­å¢ƒï¼‰...")
        words = await llm.context_translate_batch(words)
    
    # æ­¥éª¤2ï¼šè¯ä¹‰åˆ†æï¼ˆåŸºäºç¿»è¯‘ç»“æœè¿›è¡Œæ ¡å¯¹ã€å®¡æŸ¥ã€è¯­ä¹‰åˆ†æï¼Œç»™å‡ºæœ€ç»ˆè¯‘åï¼‰
    LogHelper.info("å³å°†å¼€å§‹æ‰§è¡Œ [è¯ä¹‰åˆ†æ]ï¼ˆç¬¬äºŒé˜¶æ®µï¼šæ ¡å¯¹å®¡æŸ¥ï¼Œç¡®å®šæœ€ç»ˆè¯‘åï¼‰...")
    words = await llm.surface_analysis_batch(words, fake_name_mapping)
    words = remove_words_by_type(words, "")

    # æ­¥éª¤3ï¼šé—®é¢˜ä¿®å¤ï¼ˆç¬¬ä¸‰é˜¶æ®µï¼šæ£€æµ‹å¹¶ä¿®å¤é—®é¢˜è¯æ¡ï¼‰
    LogHelper.info("")
    LogHelper.info("å³å°†å¼€å§‹æ‰§è¡Œ [é—®é¢˜ä¿®å¤]ï¼ˆç¬¬ä¸‰é˜¶æ®µï¼šæ£€æµ‹é—®é¢˜è¯æ¡ï¼Œè‡ªåŠ¨ä¿®å¤ï¼‰...")
    words = await llm.fix_translation_batch(words)

    # è°ƒè¯•åŠŸèƒ½
    TestHelper.save_surface_analysis_log(words, "log_surface_analysis.log")
    TestHelper.check_result_duplication(words, "log_result_duplication.log")
    TestHelper.save_context_translate_log(words, "log_context_translate.log")

    # è¿˜åŸä¼ªå
    for word in words:
        for k, v in fake_name_mapping.items():
            word.context_summary = word.context_summary.replace(v, k)
            word.context = [line.replace(v, k) for line in word.context]
            word.context_translation = [line.replace(v, k) for line in word.context_translation]

    # å°†ç»“æœå†™å…¥æ–‡ä»¶
    LogHelper.info("")
    file_manager.write_result_to_file(words, language)

    # æ‰§è¡Œç»“æœæ£€æŸ¥
    LogHelper.info("")
    from module.ResultChecker import ResultChecker
    checker = ResultChecker(words, language)
    checker.check_all()

    # ç­‰å¾…ç”¨æˆ·é€€å‡º
    LogHelper.info("")
    LogHelper.info("å·¥ä½œæµç¨‹å·²ç»“æŸ ... è¯·æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶ ...")
    LogHelper.info("")
    LogHelper.info("")
    os.system("pause")

# æ¥å£æµ‹è¯•
async def test_api(llm: LLM) -> None:
    # è®¾ç½®è¯·æ±‚é™åˆ¶å™¨
    llm.set_request_limiter()

    # ç­‰å¾…æ¥å£æµ‹è¯•ç»“æœ
    if await llm.api_test():
        LogHelper.print("")
        LogHelper.info("æ¥å£æµ‹è¯• [green]æ‰§è¡ŒæˆåŠŸ[/] ...")
    else:
        LogHelper.print("")
        LogHelper.warning("æ¥å£æµ‹è¯• [red]æ‰§è¡Œå¤±è´¥[/], è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ ...")

    LogHelper.print("")
    os.system("pause")
    os.system("cls")

# æ‰“å°åº”ç”¨ä¿¡æ¯
def print_app_info(config: SimpleNamespace, version: str) -> None:
    LogHelper.print()
    LogHelper.print()
    LogHelper.rule(f"ğŸ“š BookTerm Gacha {version}", style = "light_goldenrod2")
    LogHelper.rule("[blue]An LLM-Powered Agent for Book Terminology Extraction", style = "light_goldenrod2")
    LogHelper.rule("ä¸“ä¸ºä¹¦ç±ï¼ˆEPUB/TXT/MDï¼‰ä¼˜åŒ–çš„ LLM æœ¯è¯­è¡¨ç”Ÿæˆå·¥å…·", style = "light_goldenrod2")
    LogHelper.print()

    table = Table(
        box = box.ASCII2,
        expand = True,
        highlight = True,
        show_lines = True,
        show_header = False,
        border_style = "light_goldenrod2",
    )
    table.add_column("", style = "white", ratio = 2, overflow = "fold")
    table.add_column("", style = "white", ratio = 5, overflow = "fold")

    rows = []
    rows.append(("æ¨¡å‹åç§°", str(config.model_name)))
    rows.append(("æ¥å£å¯†é’¥", str(config.api_key)))
    rows.append(("æ¥å£åœ°å€", str(config.base_url)))
    rows.append(("ç½‘ç»œè¯·æ±‚è¶…æ—¶æ—¶é—´", f"{config.request_timeout} ç§’"))
    rows.append(("ç½‘ç»œè¯·æ±‚é¢‘ç‡é˜ˆå€¼", f"{config.request_frequency_threshold} æ¬¡/ç§’"))
    rows.append(("å‚è€ƒæ–‡æœ¬ç¿»è¯‘æ¨¡å¼", "æ–°æµç¨‹ï¼šå…ˆç¿»è¯‘ååˆ†æï¼ˆå¼ºåˆ¶å¯ç”¨ï¼‰"))

    for row in rows:
        table.add_row(*row)
    LogHelper.print(table)

    LogHelper.print()
    LogHelper.print("è¯·ç¼–è¾‘ [green]config.json[/] æ–‡ä»¶æ¥ä¿®æ”¹åº”ç”¨è®¾ç½® ...")
    LogHelper.print()

# æ‰“å°èœå•
def print_menu_main() -> int:
    LogHelper.print("è¯·é€‰æ‹©åŠŸèƒ½ï¼š")
    LogHelper.print("")
    LogHelper.print("\t--> 1. å¼€å§‹å¤„ç† [green]ä¸­æ–‡æ–‡æœ¬[/]")
    LogHelper.print("\t--> 2. å¼€å§‹å¤„ç† [green]è‹±æ–‡æ–‡æœ¬[/]")
    LogHelper.print("\t--> 3. å¼€å§‹å¤„ç† [green]æ—¥æ–‡æ–‡æœ¬[/]")
    LogHelper.print("\t--> 4. å¼€å§‹å¤„ç† [green]éŸ©æ–‡æ–‡æœ¬[/]")
    LogHelper.print("\t--> 5. å¼€å§‹æ‰§è¡Œ [green]æ¥å£æµ‹è¯•[/]")
    LogHelper.print("")
    choice = int(Prompt.ask("è¯·è¾“å…¥é€‰é¡¹å‰çš„ [green]æ•°å­—åºå·[/] æ¥ä½¿ç”¨å¯¹åº”çš„åŠŸèƒ½ï¼Œé»˜è®¤ä¸º [green][3][/] ",
        choices = ["1", "2", "3", "4", "5"],
        default = "3",
        show_choices = False,
        show_default = False
    ))
    LogHelper.print("")

    return choice

# ä¸»å‡½æ•°
async def begin(llm: LLM, ner: NER, file_manager: FileManager, config: SimpleNamespace, version: str) -> None:
    choice = -1
    while choice not in (1, 2, 3, 4):
        print_app_info(config, version)

        choice = print_menu_main()
        if choice == 1:
            await process_text(llm, ner, file_manager, config, NER.Language.ZH)
        elif choice == 2:
            await process_text(llm, ner, file_manager, config, NER.Language.EN)
        elif choice == 3:
            await process_text(llm, ner, file_manager, config, NER.Language.JA)
        elif choice == 4:
            await process_text(llm, ner, file_manager, config, NER.Language.KO)
        elif choice == 5:
            await test_api(llm)

# ä¸€äº›åˆå§‹åŒ–æ­¥éª¤
def load_config() -> tuple[LLM, NER, FileManager, SimpleNamespace, str]:
    global SCORE_THRESHOLD, MAX_DISPLAY_LENGTH
    
    with LogHelper.status("æ­£åœ¨åˆå§‹åŒ– [green] BookTerm Gacha [/] å¼•æ“ ..."):
        config = SimpleNamespace()
        version = ""

        try:
            # ä¼˜å…ˆä½¿ç”¨å¼€å‘ç¯å¢ƒé…ç½®æ–‡ä»¶
            if not os.path.isfile("config_dev.json"):
                path = "config.json"
            else:
                path = "config_dev.json"

            # è¯»å–é…ç½®æ–‡ä»¶
            with open(path, "r", encoding = "utf-8-sig") as reader:
                for k, v in json.load(reader).items():
                    setattr(config, k, v[0])

            # è¯»å–ç‰ˆæœ¬å·æ–‡ä»¶
            with open("version.txt", "r", encoding = "utf-8-sig") as reader:
                version = reader.read().strip()
        except Exception:
            LogHelper.error("é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ ...")

        # ============== ä»é…ç½®åŠ è½½å…¨å±€å‚æ•° ==============
        # ç½®ä¿¡åº¦é˜ˆå€¼
        SCORE_THRESHOLD = getattr(config, 'score_threshold', 0.60)
        # æœ¯è¯­æœ€å¤§æ˜¾ç¤ºé•¿åº¦
        MAX_DISPLAY_LENGTH = getattr(config, 'max_display_length', 32)
        # Word ç±»çš„ä¸Šä¸‹æ–‡é‡‡æ ·é…ç½®
        Word.set_config(
            max_context_samples=getattr(config, 'max_context_samples', 10),
            tokens_per_sample=getattr(config, 'tokens_per_sample', 512)
        )

        # åˆå§‹åŒ– LLM å¯¹è±¡
        llm = LLM(config)
        llm.load_prompt()
        llm.load_llm_config()

        # åˆå§‹åŒ– NER å¯¹è±¡
        ner = NER()
        ner.load_blacklist()
        # è®¾ç½® NER ç›®æ ‡å®ä½“ç±»å‹ï¼ˆä»é…ç½®åŠ è½½ï¼‰
        ner_target_types = getattr(config, 'ner_target_types', ["PER", "LOC"])
        ner.set_target_types(ner_target_types)

        # åˆå§‹åŒ– FileManager å¯¹è±¡ï¼ˆä¼ å…¥ç®€ç¹è½¬æ¢é…ç½®ï¼‰
        traditional_chinese_enable = getattr(config, 'traditional_chinese_enable', False)
        file_manager = FileManager(
            traditional_chinese_enable=traditional_chinese_enable
        )
        
        # æ‰“å°é…ç½®çŠ¶æ€
        LogHelper.info(f"ç½®ä¿¡åº¦é˜ˆå€¼: {SCORE_THRESHOLD}")
        LogHelper.info(f"æœ¯è¯­æœ€å¤§é•¿åº¦: {MAX_DISPLAY_LENGTH}")
        LogHelper.info(f"ä¸Šä¸‹æ–‡é‡‡æ ·æ•°: {Word.MAX_CONTEXT_SAMPLES}")
        LogHelper.info(f"æ¯æ ·æœ¬Tokenæ•°: {Word.TOKENS_PER_SAMPLE}")
        LogHelper.info(f"NERç›®æ ‡ç±»å‹: {', '.join(ner_target_types)}")
        if traditional_chinese_enable:
            LogHelper.info("ç¹ä½“ä¸­æ–‡è¾“å‡ºå·²å¯ç”¨ ...")

    return llm, ner, file_manager, config, version

# ç¡®ä¿ç¨‹åºå‡ºé”™æ—¶å¯ä»¥æ•æ‰åˆ°é”™è¯¯æ—¥å¿—
async def main() -> None:
    try:
        # æ³¨å†Œå…¨å±€å¼‚å¸¸è¿½è¸ªå™¨
        install()

        # åŠ è½½é…ç½®
        llm, ner, file_manager, config, version = load_config()

        # å¼€å§‹å¤„ç†
        await begin(llm, ner, file_manager, config, version)
    except EOFError:
        LogHelper.error("EOFError - ç¨‹åºå³å°†é€€å‡º ...")
    except KeyboardInterrupt:
        LogHelper.error("KeyboardInterrupt - ç¨‹åºå³å°†é€€å‡º ...")
    except Exception as e:
        LogHelper.error(f"{LogHelper.get_trackback(e)}")
        LogHelper.print()
        LogHelper.error("å‡ºç°ä¸¥é‡é”™è¯¯ï¼Œç¨‹åºå³å°†é€€å‡ºï¼Œé”™è¯¯ä¿¡æ¯å·²ä¿å­˜è‡³æ—¥å¿—æ–‡ä»¶ ...")
        LogHelper.print()
        os.system("pause")

# å…¥å£å‡½æ•°
if __name__ == "__main__":
    asyncio.run(main())