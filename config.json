{
    "activate_platform": 1,
    "platforms": [
        {
            "id": 0,
            "name": "智谱GLM-4.6v-flash(免费)",
            "api_url": "https://open.bigmodel.cn/api/paas/v4",
            "api_key": [

            ],
            "model": "glm-4.6v-flash",
            "thinking": true,
            "top_p": 0.95,
            "temperature": 0.05,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0,
            "description": "智谱免费模型，支持深度思考"
        },
        {
            "id": 1,
            "name": "NVIDIA-DeepSeek-V3.2",
            "api_url": "https://integrate.api.nvidia.com/v1",
            "api_key": [

            ],
            "model": "deepseek-ai/deepseek-v3.2",
            "thinking": true,
            "top_p": 0.95,
            "temperature": 0.95,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0,
            "description": "NVIDIA Build DeepSeek V3.2，17个API Key轮询"
        },
        {
            "id": 2,
            "name": "魔塔-DeepSeek-V3.2",
            "api_url": "https://api-inference.modelscope.cn/v1/",
            "api_key": [

            ],
            "model": "deepseek-ai/DeepSeek-V3.2",
            "thinking": true,
            "top_p": 0.95,
            "temperature": 0.95,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0,
            "description": "阿里云百炼 ModelScope DeepSeek V3.2，5个API Key轮询"
        },
        {
            "id": 3,
            "name": "魔塔-DeepSeek-R1-0528",
            "api_url": "https://api-inference.modelscope.cn/v1/",
            "api_key": [

            ],
            "model": "deepseek-ai/DeepSeek-R1-0528",
            "thinking": true,
            "top_p": 0.95,
            "temperature": 0.95,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0,
            "description": "阿里云百炼 ModelScope DeepSeek R1"
        }
    ],
    "count_threshold": [
        2,
        "出现次数阈值，出现次数低于此值的词语会被过滤掉以节约时间。"
    ],
    "request_timeout": [
        1800,
        "网络请求超时时间，如果频繁出现 timeout 字样的网络错误，可以调大这个值。"
    ],
    "request_frequency_threshold": [
        6,
        "网络请求频率阈值，单位为 次/秒，值可以小于 1，如果频繁出现 429 代码的网络错误，可以调小这个值。",
        "使用 llama.cpp 运行的本地模型时，将根据 llama.cpp 的配置调整自动设置，无需手动调整这个值。",
        "使用多API轮询时建议设为 5-10，充分利用并发能力。"
    ],
    "max_concurrent_requests": [
        600,
        "最大并发请求数，同时进行的请求数量上限。",
        "使用多API轮询时可以适当增加（如 5-10），充分利用多Key并发。",
        "此值与 request_frequency_threshold 共同控制并发：并发数 = min(max_concurrent_requests, request_frequency_threshold)。"
    ],
    "traditional_chinese_enable": [
        false,
        "是否启用繁体中文输出，启用后所有中文输出将转换为繁体中文。"
    ],
    "score_threshold": [
        0.60,
        "置信度阈值，NER 识别置信度低于此值的词语会被过滤掉。",
        "建议范围 0.50-0.70，值越低识别的词语越多但准确率可能下降。"
    ],
    "max_display_length": [
        32,
        "术语最大显示长度，超过此长度的术语会被过滤掉。",
        "用于过滤异常的超长术语。"
    ],
    "max_context_samples": [
        5,
        "上下文采样数量，从同一词语出现的不同位置采样多少条上下文。",
        "值越大提供的语境越丰富，但会增加 token 消耗。建议 5-15。"
    ],
    "tokens_per_sample": [
        512,
        "每条采样上下文的最大 token 数。",
        "用于控制单条上下文的长度，建议 256-1024。"
    ],
    "ner_target_types": [
        ["PER", "LOC"],
        "NER 阶段保留的实体类型列表。",
        "可选值：PER(人名)、LOC(地点)、ORG(组织)、PRD(产品/作品)。",
        "默认只保留 PER 和 LOC，可大幅减少后续 LLM 调用次数。",
        "如需保留全部类型，设为 [\"PER\", \"LOC\", \"ORG\", \"PRD\"]。"
    ],
    "task_timeout_threshold": [
        600,
        "单任务超时阈值（秒），超过此时间将触发上下文限缩策略。",
        "当某个词条处理时间过长时，会自动减少参考上下文数量并重试。",
        "建议范围 300-600 秒，默认 430 秒。"
    ]
}