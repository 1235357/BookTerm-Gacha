{
    "api_key": [
        "0d216acbbaea857900f5964e8869375b.LYjkb7VFAGRZKwHa",
        "接口密钥，从接口平台方获取，使用在线接口时一定要设置正确。"
    ],
    "base_url": [
        "https://open.bigmodel.cn/api/paas/v4",
        "请求地址，从接口平台方获取，使用在线接口时一定要设置正确。"
    ],
    "model_name": [
        "glm-4.6v-flash",
        "模型名称，从接口平台方获取，使用在线接口时一定要设置正确。"
    ],
    "count_threshold": [
        2,
        "出现次数阈值，出现次数低于此值的词语会被过滤掉以节约时间。"
    ],
    "request_timeout": [
        1800,
        "网络请求超时时间，如果频繁出现 timeout 字样的网络错误，可以调大这个值。"
    ],
    "request_frequency_threshold": [
        5,
        "网络请求频率阈值，单位为 次/秒，值可以小于 1，如果频繁出现 429 代码的网络错误，可以调小这个值。",
        "使用 llama.cpp 运行的本地模型时，将根据 llama.cpp 的配置调整自动设置，无需手动调整这个值。",
        "使用 火山引擎 等不限制并发数的在线接口时可以调大这个值。"
    ],
    "traditional_chinese_enable": [
        false,
        "是否启用繁体中文输出，启用后所有中文输出将转换为繁体中文。"
    ],
    "score_threshold": [
        0.60,
        "置信度阈值，NER 识别置信度低于此值的词语会被过滤掉。",
        "建议范围 0.50-0.70，值越低识别的词语越多但准确率可能下降。"
    ],
    "max_display_length": [
        32,
        "术语最大显示长度，超过此长度的术语会被过滤掉。",
        "用于过滤异常的超长术语。"
    ],
    "max_context_samples": [
        5,
        "上下文采样数量，从同一词语出现的不同位置采样多少条上下文。",
        "值越大提供的语境越丰富，但会增加 token 消耗。建议 5-15。"
    ],
    "tokens_per_sample": [
        512,
        "每条采样上下文的最大 token 数。",
        "用于控制单条上下文的长度，建议 256-1024。"
    ],
    "ner_target_types": [
        ["PER", "LOC"],
        "NER 阶段保留的实体类型列表。",
        "可选值：PER(人名)、LOC(地点)、ORG(组织)、PRD(产品/作品)。",
        "默认只保留 PER 和 LOC，可大幅减少后续 LLM 调用次数。",
        "如需保留全部类型，设为 [\"PER\", \"LOC\", \"ORG\", \"PRD\"]。"
    ]
}